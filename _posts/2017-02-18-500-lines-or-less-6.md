---
layout:     post
title:      "500 Lines or Less Chapter 5: A Web Crawler With asyncio Coroutines 翻译"
category:   Translation
tags:       Python Translation
---

* content
{:toc}

[500 Lines or Less](https://github.com/aosabook/500lines)

作者是来自纽约A. Jesse Jiryu Davis是的工程师A. Jesse Jiryu Davis。 他贡献asyncio和Tornado在http://reedysqua.re上。和Python之父Guido van Rossum。 Guido的博客地址是http://www.python.org/~guido/。

## 引言
经典计算机科学强调的是高效算法，尽可能快地完成计算。但许多网络程序更多的时间不是用在计算，而是缓慢的打开许多链接，或者处理一些不常见的情况。其中的挑战是：高效地等待大量的网络事件。当前处理这个问题的方法是异步I/O。

本章介绍一个简单的网络爬虫。爬虫也是一个异步应用程序，它等待很多响应但计算量很少。一次获取的页面越多，越早完成程序。如果它将线程分配给每个请求，则随着并发请求数量的增加，它将耗尽内存或其他线程的相关资源，直到它耗尽套接字。通过使用异步I/O避开对线程的需求。

我们在三个阶段提出的例子。首先，展示一个异步事件循环，并使用带回调的事件循环的爬虫：这非常有效，但是扩展到更复杂的问题将导致无法管理代码。第二，因此表明Python协程是高效和可扩展的。我们使用生成器函数在Python中实现简单的协同程序。在第三阶段，使用来自Python标准库里的`asyncio`的协同程序，并使用异步队列来协调它们。

## 任务

网络抓取工具查找和下载网站上的所有网页，也可以对其进行归档或编制索引。 从根URL开始，它获取每个页面，解析它的链接到没打开的的页面，并将它们添加到队列中。当它获取没打开的的页面的链接并且队列为空时，停止工作。

我们可以通过同时下载多页来加快这个过程。当爬虫找到新链接时，它会在单独的套接字上启动新页面的同步抓取操作。在响应到达时解析响应，向队列添加新链接。可能会出现一些减少返回的点，其中过多的并发性降低性能，因此我们限制并发请求的数量，并将剩余的链接留在队列中，直到请求完成。

## 传统的方法

我们如何实现爬行器并发？ 传统的方法里，我们创建一个线程池。每个线程将负责通过套接字一页页下载。例如，要从`xkcd.com`下载一个页面：

```python
def fetch(url):
    sock = socket.socket()
    sock.connect(('xkcd.com', 80))
    request = 'GET {} HTTP/1.0\r\nHost: xkcd.com\r\n\r\n'.format(url)
    sock.send(request.encode('ascii'))
    response = b''
    chunk = sock.recv(4096)
    while chunk:
        response += chunk
        chunk = sock.recv(4096)

    # Page is now downloaded.
    links = parse_links(response)
    q.add(links)
```

默认情况下，套接字操作是阻塞的：当线程调用leisi connect或recv的方法时，它会停止直到操作完成。因此，为了一次下载多个页面，我们需要许多线程。一个复杂的应用程序通过将线程池中的空闲线程保持在线程池中，然后将它们检出以重用它们用于后续任务来摊销线程创建的成本;它对连接池中的套接字执行相同操作。

但是线程成本昂贵的，而且操作系统对进程，操作系统对一个进程、一个用户、一台机器能使用线程做了不同的硬性限制。在Jesse的系统中，Python线程需要大约50k的内存，并且启动数以万计的线程就会导致失败。如果我们在并发套接字上扩展到数万个并发操作，我们就会耗尽线程用完套接字。每个线程的开销和系统的限制就是这种方式的瓶颈所在。

在他其有影响的文章"The C10K problem"中，Dan Kegel概述了I/O并发的多线程的局限性。

>	如今网络服务器要同时处理一万个客户端，你不这么觉得吗？毕竟，网络规模已经很大了。

凯格尔在1999年创造了"C10K"这个术语。一万个连接现在听起来很好，但其实也只是在尺寸上变化。那时，使用C10K的每个连接的线程是不切实际的。现在是更高的数量级。事实上，我们的网络爬虫使用线程正常工作。然而对于非常大规模的应用程序，有成千上万的连接，仍然存在上限：超过该限制大多数系统仍然可以创建套接字，在耗尽了线程的时候。我们应该如何克服这一点？

## 异步

异步I/O框架使用非阻塞套接字在单个线程上执行并行操作。在我们的异步爬虫中，在我们连接到服务器之前设置套接字无阻塞：

```python
sock = socket.socket()
sock.setblocking(False)
try:
    sock.connect(('xkcd.com', 80))
except BlockingIOError:
    pass
```
令人恼火的是非阻塞套接字抛出异常，即使它正常工作。这个异常复制了底层C函数讨人厌的行为，它将`errno`设置为`EINPROGRESS`来告诉你它已经开始工作。

现在我们的爬虫需要一种方法来确定连接何时建立，才可以发送HTTP请求。 我们可以写一个简单的循环：

```python 
request = 'GET {} HTTP/1.0\r\nHost: xkcd.com\r\n\r\n'.format(url)
encoded = request.encode('ascii')

while True:
    try:
        sock.send(encoded)
        break  # Done.
    except OSError as e:
        pass

print('sent')
```

这种方法不仅耗CPU，而且不能有效地等待多个套接字。 过去BSD Unix的解决这个问题的解决方案是选择一个C函数，等待一个事件发生在一个非阻塞的套接字或一个小数组。 如今，对具有大量连接的互联网应用的需求导致了诸如'poll'的替换，然后是BSD的`kqueue`和Linux上的`epoll`。这些类似于`select`的API，但是对于非常大量的连接非常有效。

Python 3.4的`DefaultSelector`使用系统上可用的最佳选择函数。要注册有关网络I/O 的提醒，我们需要创建一个非阻塞套接字并使用默认选择器注册它：

```python 
from selectors import DefaultSelector, EVENT_WRITE

selector = DefaultSelector()

sock = socket.socket()
sock.setblocking(False)
try:
    sock.connect(('xkcd.com', 80))
except BlockingIOError:
    pass

def connected():
    selector.unregister(sock.fileno())
    print('connected!')

selector.register(sock.fileno(), EVENT_WRITE, connected)
```

我们忽略了这个伪错误并调用`selector.register`，传递套接字的文件描述符和一个常量，代表我们正在等待的事件。 要在连接建立时通知，我们传递`EVENT_WRITE`：也就是说，我们想知道套接字何时是"可写的"。 我们还传递一个Python函数，`connected`当事件发生时运行。该函数称为回调。

当选择器接收到它们时，我们在一个循环中处理I/O通知：

```python 
def loop():
    while True:
        events = selector.select()
        for event_key, event_mask in events:
            callback = event_key.data
            callback()
```

这个`connected`回调被存储为`event_key.data`，一旦非阻塞套接字被连接便检索和执行。

不像前面的快速循环，这里`select`暂停调用，等待下一个I/O事件。然后循环运行等待这些事件的回调。尚未完成的操作将保持pending，直到进到下一个事件循环时执行。

到目前为止我们展示了什么？我们展示了如何开始I/O操作和执行回调。异步框架建立展示的两个特性（非阻塞套接字和事件循环），以在单个线程上运行并发操作。

我们在这里实现了“并发”，但不是传统上被称为“并行性”。也就是说，我们构建了一个重叠I/O的小系统。它能够开始新的行动，而其他也在操作。它实际上并不利用多个核来并行执行计算。但是，这个系统是为I/O绑定的问题设计的，而不是CPU限制的。

因此，我们的事件循环在并发I/O方面是高效的，因为它不会将线程资源分配给每个连接。但在我们继续前，纠正一个常见的误解，即异步的速度比多线程快。通常不是，事实上在Python中，像我们这样的事件循环比服务少数非常活跃的连接的多线程慢。在没有全局解释器锁的运行时，线程在这样的工作负载上表现更好。“什么是异步，它如何工作，什么时候该用它？”一文中指出了异步所适用和不适用的场景。

## 回调

目前我们已经构建异步框架，将如何继续构建一个网络爬虫？ 即使写一个简单的URL爬取也是痛苦的是痛苦。

我们先从全局的URL集合开始，我们看到的URL：

```python 
urls_todo = set(['/'])
seen_urls = set(['/'])
```

`seen_urls`集里包括`urls_todo`加完成的URL。这两个集合用于初始化URL“/”。

获取页面将需要一系列回调。`connected`在连接套接字时触发回调，并向服务器发送GET请求。但是它必须等待响应，所以需要注册另一个回调。 如果，当回调触发时，它不能读取完整的响应，它再次注册，等等。

让我们把这些回调放在一个`Fetcher`对象中，它需要一个URL，一个套接字，还需要一个地方保存返回的字节：

```python 
class Fetcher:
    def __init__(self, url):
        self.response = b''  # Empty array of bytes.
        self.url = url
        self.sock = None
```

我们开始调用`Fetcher.fetch`

```python 
    # Method on Fetcher class.
    def fetch(self):
        self.sock = socket.socket()
        self.sock.setblocking(False)
        try:
            self.sock.connect(('xkcd.com', 80))
        except BlockingIOError:
            pass

        # Register next callback.
        selector.register(self.sock.fileno(),
                          EVENT_WRITE,
                          self.connected)
```

`fetch`函数开始连接套接字。 但请注意，该方法在建立连接之前返回。 它必须将控制权返回到事件循环，以等待连接。为了理解为什么，想象我们的整个应用程序的结构：

```python 
# Begin fetching http://xkcd.com/353/
fetcher = Fetcher('/353/')
fetcher.fetch()

while True:
    events = selector.select()
    for event_key, event_mask in events:
        callback = event_key.data
        callback(event_key, event_mask)
```

所有事件提醒在调用`select`时在事件循环中处理。 因此，`fetch`必须手动控制到事件循环，以便程序知道套接字何时连接。 只有这样，循环才会运行连接的回调，这在`fetch`的结尾处注册。

以下是实现`connected`的代码：

```python 
    # Method on Fetcher class.
    def connected(self, key, mask):
        print('connected!')
        selector.unregister(key.fd)
        request = 'GET {} HTTP/1.0\r\nHost: xkcd.com\r\n\r\n'.format(self.url)
        self.sock.send(request.encode('ascii'))

        # Register the next callback.
        selector.register(key.fd,
                          EVENT_READ,
                          self.read_response)
```

该方法发送`GET`请求。真正的应用程序将检查`send`的返回值，以防整个消息不能立即发送。不过这里的程序不复杂。 它调用`send`，然后等待响应。当然，它必须注册另一个回调，并放弃对事件循环的控制。 最后一个回调`read_response`处理服务器的回应：

```python 
    # Method on Fetcher class.
    def read_response(self, key, mask):
        global stopped

        chunk = self.sock.recv(4096)  # 4k chunk size.
        if chunk:
            self.response += chunk
        else:
            selector.unregister(key.fd)  # Done reading.
            links = self.parse_links()

            # Python set-logic:
            for link in links.difference(seen_urls):
                urls_todo.add(link)
                Fetcher(link).fetch()  # <- New Fetcher.

            seen_urls.update(links)
            urls_todo.remove(self.url)
            if not urls_todo:
                stopped = True
```

每次选择器看到套接字是“可读的”时，就会执行回调，这意味着：套接字有数据或已关闭。

回调从套接字请求最多4千字节的数据。如果更少，`chunk`包含任何可用的数据。如果有更多，`chunk`有4千字节长，并且套接字保持可读，因此事件循环在下一个响应时再次运行这个回调。当响应完成时，服务器已关闭套接字并且`chunk`为空。

`parse_links`方法返回一组URL。我们为每个新网址开始一个新的抓取器，没有并发上限。注意使用回调的异步编程的一个很好的功能：我们不需要围绕共享数据的变化的互斥，例如当我们添加链接到`seen_urls`。 没有优先的多任务，所以我们不能在代码中的任意点被打断。

我们添加一个全局`stop`变量，并使用它来控制循环：

```python 
stopped = False

def loop():
    while not stopped:
        events = selector.select()
        for event_key, event_mask in events:
            callback = event_key.data
            callback()
```

一旦下载所有页面，抓取器会停止全局事件循环，程序退出。

这个例子使异步问题变得很简单。我们需要一些方法来表达一系列计算和I/O操作，并且调度多个一系列操作以并发运行。但是没有线程，一系列操作不能被收集到单个函数中：每当一个函数开始I/O操作时，它显式地保存将来所需要的任何状态，然后返回。 你可以思考以下如何编写这个状态节省代码。

让我们来解释一下。考虑我们如何简单地在一个具有常规阻塞套接字的线程上获取一个URL：

```python 
# Blocking version.
def fetch(url):
    sock = socket.socket()
    sock.connect(('xkcd.com', 80))
    request = 'GET {} HTTP/1.0\r\nHost: xkcd.com\r\n\r\n'.format(url)
    sock.send(request.encode('ascii'))
    response = b''
    chunk = sock.recv(4096)
    while chunk:
        response += chunk
        chunk = sock.recv(4096)

    # Page is now downloaded.
    links = parse_links(response)
    q.add(links)
```

这个函数在一个套接字操作和下一个套接字操作之间记录了什么？它有套接字，一个URL和积累的`response`。在线程上运行的函数使用编程语言的基本特性将该临时状态存储在其堆栈中的局部变量中。该函数还有一个“继续”，即它计划在I/O完成后执行的代码。运行时通过存储线程的指令指针来记住。您不必考虑恢复这些局部变量和I/O后的继续。它是内置的语言。

但是使用基于回调的异步框架，这些语言功能并没有什么帮助。在等待I/O时，函数必须显式地保存其状态，因为该函数在I/O完成之前返回并丢失其堆栈。Fetcher实例中我们的基于回调存储的`sock`和`response`作为`self`的属性。代替指令指针，它通过注册`connected`的回调和`read_response`来存储它的连续。随着应用程序的功能增加，我们在回调中手动保存状态的复杂性也在增加。

更糟的是，如果回调引发异常，在调度链中的下一个回调之前会发生什么？所以在`parse_links`方法上做的并不好，它抛出解析HTML的异常：

```python 
Traceback (most recent call last):
  File "loop-with-callbacks.py", line 111, in <module>
    loop()
  File "loop-with-callbacks.py", line 106, in loop
    callback(event_key, event_mask)
  File "loop-with-callbacks.py", line 51, in read_response
    links = self.parse_links()
  File "loop-with-callbacks.py", line 67, in parse_links
    raise Exception('parse error')
Exception: parse error
```

堆跟踪仅显示事件循环正在运行回调。我们不记得是什么导致的错误。链条在两端都断了：我们忘了我们去哪里，我们为何而来。这种上下文的丢失被称为“stack ripping”，并且在许多情况下它混淆了研究者。堆栈翻录还阻止我们为回调链安装异常处理程序，“try/except”块包装函数调用及其后代的方式.

因此，除了关于多线程和异步的相对效率的长期争论之外，还有另一个争论是更容易出错的：如果你使错误同步它们，线程就容易受到数据竞争，但是回调是固执的调试由于堆栈翻录。

## 协同

可以编写异步代码，将回调的效率与多线程编程的经典好看结合起来。这种组合是通过“协同程序”的模式来实现。使用Python3.4的标准asyncio库和“aiohttp”的包，在协程中非常直接获取URL：

```python 
    @asyncio.coroutine
    def fetch(self, url):
        response = yield from self.session.get(url)
        body = yield from response.read()
```

它也是可扩展的。与每个线程的50k内存和操作系统对线程的硬限制相比，Python协程在Jesse的系统上只需要3k的内存。 Python可以轻松启动数十万个协程。

协同程序的概念，可以追溯到很早以前的计算机科学：它是一个可以暂停和恢复的子程序。而线程是由操作系统抢占式多任务，协同多任务协作：他们选择何时暂停，以及哪个协程运行下一步。

有很多协同程序的实现，在Python就有好几个。Python3.4中的标准“asyncio”库中的协程是基于生成器，Future类和“yield from”语句构建的。从Python3.5开始，协程是语言本身的一个本地特性;然而，了解协同程序，，使用预先存在的语言设施，是解决Python3.5的本地协同程序的基础。

为了解释Python3.4的基于生成器的协同程序，我们将介绍一些生成器以及它们如何在asyncio中用作协程，并且相信你会喜欢阅读它。一旦我们解释了基于生成器的协同程序，我们将使用它们在我们的异步Web爬虫。

## Python生成器如何工作

在掌握Python生成器之前，您必须了解常规Python函数的工作原理。通常，当Python函数调用子例程时，子例程保留控制，直到返回或抛出异常。然后控制权返回给调用者：

```python 
>>> def foo():
...     bar()
...
>>> def bar():
...     pass
```

标准的Python解释器是用C编写的。执行Python函数的C函数被称`PyEval_EvalFrameEx`。它需要一个Python栈框架对象，并在框架的上下文中评估Python字节码。 这里是foo的字节码：

```python 
>>> import dis
>>> dis.dis(foo)
  2           0 LOAD_GLOBAL              0 (bar)
              3 CALL_FUNCTION            0 (0 positional, 0 keyword pair)
              6 POP_TOP
              7 LOAD_CONST               0 (None)
             10 RETURN_VALUE
```

`foo`函数加载`bar`到其堆中调用，然后从堆中弹出其返回值，将`None`加载到堆栈，并返回`None`。

当`PyEval_EvalFrameEx`遇到`CALL_FUNCTION`字节码时，它会递归创建一个新的`Python`栈：也就是说，它使用新的堆递归调用`PyEval_EvalFrameEx`，用于执行`bar`。

了解Python堆栈帧在堆内存中分配是很重要的！Python解释器是一个C程序，所以它的堆框架也是正常的堆栈帧。但Python堆框架是在建立堆上。 除此之外，这意味着Python堆框架可以为其函数调用。要以交互方式查看，从`bar`保存当前的框架

```python 
>>> import inspect
>>> frame = None
>>> def foo():
...     bar()
...
>>> def bar():
...     global frame
...     frame = inspect.currentframe()
...
>>> foo()
>>> # The frame was executing the code for 'bar'.
>>> frame.f_code.co_name
'bar'
>>> # Its back pointer refers to the frame for 'foo'.
>>> caller_frame = frame.f_back
>>> caller_frame.f_code.co_name
'foo'
```

![](/images/python/500lines1.png)

该阶段是设置Python生成器，使用相同的构建块 - 代码对象和堆。

这是一个生成器函数：

```python 
>>> def gen_fn():
...     result = yield 1
...     print('result of yield: {}'.format(result))
...     result2 = yield 2
...     print('result of 2nd yield: {}'.format(result2))
...     return 'done'
...     
```

当Python将`gen_fn`编译为字节码时，它会看到`yield`状态，并且知道`gen_fn`是一个生成器函数，而不是一个常规函数。 它设置一个标志记住：

```python 
>>> # The generator flag is bit position 5.
>>> generator_bit = 1 << 5
>>> bool(gen_fn.__code__.co_flags & generator_bit)
True
```

当你调用一个生成器函数，Python看到生成器标志，它实际上并不运行该函数。 相反，它创建一个生成器：

```python 
>>> gen = gen_fn()
>>> type(gen)
<class 'generator'>
```

Python生成器封装了一个堆和一些代码的引用，`gen_fn`的主体：

```python 
>>> gen.gi_code.co_name
'gen_fn'
```

所有从`gen_fn`调用的生成器都指向这个相同的代码。但每个都有自己的堆。 这个堆栈帧不在任何实际堆中，它坐在栈内存等待使用：

![](/images/python/500lines2.png)

该框架具有“last instruction”指针，它是最新执行的指令。 开始时，last instruction指针是-1，表示生成器尚未开始：

```python 
>>> gen.gi_frame.f_lasti
-1
```

当我们调用`send`时，生成器到底第一个`yield`并且暂停。`send`的返回值是1，因为这是`gen`传递给`yield`表达式：

```python 
>>> gen.send(None)
1
```

生成器的指令指针现在是从开始的3个字节码，部分通过编译的Python的56个字节：

```python 
>>> gen.gi_frame.f_lasti
3
>>> len(gen.gi_code.co_code)
56
```

生成器可以在任何时候从任何函数恢复，因为它的堆实际上不在堆上：它在栈上。它在调用层次结构中的位置 不固定的，并且它不需要遵守常规函数执行的先进先出顺序。

我们可以将值“hello”发送到生成器，它成为`yield`表达式的结果，并且生成器继续，直到它产生yield 2：

```python 
>>> gen.send('hello')
result of yield: hello
2
```




## 引用

1. Guido introduced the standard asyncio library, called "Tulip" then, at PyCon 2013.

2. Even calls to send can block, if the recipient is slow to acknowledge outstanding messages and the system's buffer of outgoing data is full.

3. http://www.kegel.com/c10k.html

4. Python's global interpreter lock prohibits running Python code in parallel in one process anyway. Parallelizing CPU-bound algorithms in Python requires multiple processes, or writing the parallel portions of the code in C. But that is a topic for another day.

5. Jesse listed indications and contraindications for using async in "What Is Async, How Does It Work, And When Should I Use It?":. Mike Bayer compared the throughput of asyncio and multithreading for different workloads in "Asynchronous Python and Databases":

6. For a complex solution to this problem, see http://www.tornadoweb.org/en/stable/stack_context.html

7. The @asyncio.coroutine decorator is not magical. In fact, if it decorates a generator function and the PYTHONASYNCIODEBUG environment variable is not set, the decorator does practically nothing. It just sets an attribute, _is_coroutine, for the convenience of other parts of the framework. It is possible to use asyncio with bare generators not decorated with @asyncio.coroutine at all.

8. Python 3.5's built-in coroutines are described in PEP 492, "Coroutines with async and await syntax."

9. This future has many deficiencies. For example, once this future is resolved, a coroutine that yields it should resume immediately instead of pausing, but with our code it does not. See asyncio's Future class for a complete implementation.

10. In fact, this is exactly how "yield from" works in CPython. A function increments its instruction pointer before executing each statement. But after the outer generator executes "yield from", it subtracts 1 from its instruction pointer to keep itself pinned at the "yield from" statement. Then it yields to its caller. The cycle repeats until the inner generator throws StopIteration, at which point the outer generator finally allows itself to advance to the next instruction.

11. https://docs.python.org/3/library/queue.html

12. https://docs.python.org/3/library/asyncio-sync.html

13. The actual asyncio.Queue implementation uses an asyncio.Event in place of the Future shown here. The difference is an Event can be reset, whereas a Future cannot transition from resolved back to pending.

14. https://glyph.twistedmatrix.com/2014/02/unyielding.html