
它的堆现在包括了局部变量`result`

```python 
>>> gen.gi_frame.f_locals
{'result': 'hello'}
```

从`gen_fn`创建的其他生成器将有自己的堆和局部变量。

当我们再次调用`send`时，生成器继续通过它的第二个`yield`，并抛出特别的`StopIteration`异常结束：

```python
>>> gen.send('goodbye')
result of 2nd yield: goodbye
Traceback (most recent call last):
  File "<input>", line 1, in <module>
StopIteration: done
```

这个异常有一个值，它是生成器的返回值：字符串“done”。

## 使用生成器构建协同程序
从保存的值恢复，具有返回值。听起来很好地构建一个异步编程模型。 我们想建立一个“协同程序”：一个与程序中的其他程序一起调度的程序。 我们的协程将是Python标准“asyncio”库中的简化版本。 在asyncio中，我们将使用generator，futures和“yield from”语句。

首先，我们需要一种方法来表示协同程序正在等待的未来结果。精简的版本如下：

```python
class Future:
    def __init__(self):
        self.result = None
        self._callbacks = []

    def add_done_callback(self, fn):
        self._callbacks.append(fn)

    def set_result(self, result):
        self.result = result
        for fn in self._callbacks:
            fn(self)
```

future 初始时pending，它通过调用`set_result`解析。

让我们使用futures和coroutines来调整抓取器。通过回调来写`fetch`	

```python
class Fetcher:
    def fetch(self):
        self.sock = socket.socket()
        self.sock.setblocking(False)
        try:
            self.sock.connect(('xkcd.com', 80))
        except BlockingIOError:
            pass
        selector.register(self.sock.fileno(),
                          EVENT_WRITE,
                          self.connected)

    def connected(self, key, mask):
        print('connected!')
        # And so on....
```

`fetch`方法开始连接套接字，然后注册回调，`connected`，当套接字准备好时执行。现在我们可以将这两个步骤组合成一个协同程序

```python
    def fetch(self):
        sock = socket.socket()
        sock.setblocking(False)
        try:
            sock.connect(('xkcd.com', 80))
        except BlockingIOError:
            pass

        f = Future()

        def on_connected():
            f.set_result(None)

        selector.register(sock.fileno(),
                          EVENT_WRITE,
                          on_connected)
        yield f
        selector.unregister(sock.fileno())
        print('connected!')
```

现在`fetch`是一个生成器函数，而不是一个常规函数，因为它包含`yield`语句。我们创建一个pending future，然后让它暂停`fetch`直到套接字准备就绪。内部函数`on_connected`解析future。

但是，当future解析式，是什么重启生成器呢？ 我们需要一个协程驱动程序。 称之为“任务”：

```python
lass Task:
    def __init__(self, coro):
        self.coro = coro
        f = Future()
        f.set_result(None)
        self.step(f)

    def step(self, future):
        try:
            next_future = self.coro.send(future.result)
        except StopIteration:
            return

        next_future.add_done_callback(self.step)

# Begin fetching http://xkcd.com/353/
fetcher = Fetcher('/353/')
Task(fetcher.fetch())

loop()
```

任务通过向其发送`None`来启动`fetch`生成器。 然后`fetch`运行，直到它产生一个future，任务用`next_future`捕获。当套接字连接时，事件循环运行回调`on_connected`，它解析`future`调用`step`，恢复`fetch`。

## 用`yield from`转换协同

一旦套接字连接，我们发送HTTP GET请求并读取服务器响应。 这些步骤不再分散在回调中; 我们将它们收集到相同的生成函数中：

```python
    def fetch(self):
        # ... connection logic from above, then:
        sock.send(request.encode('ascii'))

        while True:
            f = Future()

            def on_readable():
                f.set_result(sock.recv(4096))

            selector.register(sock.fileno(),
                              EVENT_READ,
                              on_readable)
            chunk = yield f
            selector.unregister(sock.fileno())
            if chunk:
                self.response += chunk
            else:
                # Done reading.
                break
```

这个代码从套接字读取整个消息。我们如何将它从`fetch`转换为子程序？ 现在Python3的`yield from`接管了这个阶段。 它把一个生成器委托给另一个。

让我们回到我们简单的生成器示例：

```python
>>> def gen_fn():
...     result = yield 1
...     print('result of yield: {}'.format(result))
...     result2 = yield 2
...     print('result of 2nd yield: {}'.format(result2))
...     return 'done'
...     
```

要从另一个生成器调用此生成器，请使用`yield`：

```python
>>> # Generator function:
>>> def caller_fn():
...     gen = gen_fn()
...     rv = yield from gen
...     print('return value of yield-from: {}'
...           .format(rv))
...
>>> # Make a generator from the
>>> # generator function.
>>> caller = caller_fn()
```

`caller`生成器就像是`gen`，它被委托给：

```python
>>> caller.send(None)
1
>>> caller.gi_frame.f_lasti
15
>>> caller.send('hello')
result of yield: hello
2
>>> caller.gi_frame.f_lasti  # Hasn't advanced.
15
>>> caller.send('goodbye')
result of 2nd yield: goodbye
return value of yield-from: done
Traceback (most recent call last):
  File "<input>", line 1, in <module>
StopIteration
```

当`caller`yield from`gen1`，`caller`不继续往前。它的指令指针保持在15，它的`yield from`的位置，即使内部生成器`gen`从一个`yield`语句到下一个。从`caller`外部，我们无法知道它产生的值是从`caller`或从其委派的生成器。从`gen`内部，我们不能知道值是从`caller`还是从外部发送。`yield from`是一个无摩擦通道，从它的值输入和`gen`输出直到`gen`完成。

协同程序可以将工作委托给一个有`yield from`子协同程序，并从中接收工作结果。 注意，上面`caller`打印“return value of yield-from: done”。 当`gen`完成时，其返回值成为`caller`中`yield from`的值：

```python
    rv = yield from gen
```

早些时候，当我们批评基于回调的异步编程时，我们最强烈的投诉是""stack ripping"：当回调抛出异常时，堆跟踪通常是无用的。它只显示事件循环正在运行回调，而不是为什么。协程如何运行？

```python
>>> def gen_fn():
...     raise Exception('my error')
>>> caller = caller_fn()
>>> caller.send(None)
Traceback (most recent call last):
  File "<input>", line 1, in <module>
  File "<input>", line 3, in caller_fn
  File "<input>", line 2, in gen_fn
Exception: my error
```

这更有用！ 堆跟踪显示`caller_fn`在委托`gen_fn`时抛出错误。 更令人欣慰的是，我们可以将调用包装到异常处理程序中的子协程，同样使用正常的子程序：

```python
>>> def gen_fn():
...     yield 1
...     raise Exception('uh oh')
...
>>> def caller_fn():
...     try:
...         yield from gen_fn()
...     except Exception as exc:
...         print('caught {}'.format(exc))
...
>>> caller = caller_fn()
>>> caller.send(None)
1
>>> caller.send('hello')
caught uh oh
```

因此，我们用子协同因子转换逻辑，就像使用常规子程序一样。 让我们用抓取器转化一些有用的子协程。我们写一个`read`协程来接收块：

```python
def read(sock):
    f = Future()

    def on_readable():
        f.set_result(sock.recv(4096))

    selector.register(sock.fileno(), EVENT_READ, on_readable)
    chunk = yield f  # Read one chunk.
    selector.unregister(sock.fileno())
    return chunk
```

用`read_all`协程来构建`read`，获取所有的信息

```python
def read_all(sock):
    response = []
    # Read whole response.
    chunk = yield from read(sock)
    while chunk:
        response.append(chunk)
        chunk = yield from read(sock)

    return b''.join(response)
```

如果你以正确的方式倾斜，`yield from`消失，这看起来像阻塞I/O的常规函数。但事实上，`read`和`read_all`是协程。`read`暂停`read_all`直到I/O完成。当`read_all`暂停时，异步事件循环执行其他工作，并等待其他I/O事件; `read_all`在恢复事件准备好后，在下一个循环上读取的结果。

在堆根目录，`fetch`调用`read_all`：

```python
class Fetcher:
    def fetch(self):
         # ... connection logic from above, then:
        sock.send(request.encode('ascii'))
        self.response = yield from read_all(sock)
```

奇怪的是，Task类不需要修改。它和之前一样驱动外部`fetch`协同程序：

```python
Task(fetcher.fetch())
loop()
```

当`read`产生future时，任务通过`yield from`语句的通道获取，就好像future直接从`fetch`产生。 当循环解析future时，任务将其结果发送到`fetch`，并且`read`接收该值，就好像任务直接驱动`read`一样：

![](/images/python/500lines3.png)

为了完善协程，我们抛出一个错误：在等待未来时使用`yield`，当它委托给一个子协程时`yield from`。

我们利用Python生成器和迭代器之间的深层对应。 推进生成器对于调用者，与推进迭代器相同。 所以我们通过一个特殊的方法使我们的Future类可迭代：

```python
    # Method on Future class.
    def __iter__(self):
        # Tell Task to resume me here.
        yield self
        return self.result
```

future的`__iter__`方法是yield future的协程。现在我们使用以下的代码：

```python
# f is a Future.
yield f
```

用这个：

```python
# f is a Future.
yield from f
```

结果是一样的！驱动Task从其调用`send`获取future，当未来被解析时，其将新的结果发送回协同。

使用`yield from`有什么优点？为什么用`yield`等待future和用`yield from`委托到子协同更好？因为它可以自由地改变其实现而不影响调用者：它可能是一个正常的方法返回future将解析为一个值，它也可能是包含`yield from`语句的协同程序，并返回值。在任何一种情况下，调用者只需要从方法中yield，以便等待结果。

我们已经完成异步协作。我们探讨了生成器的机制，并且实现了future和task，概述了如何更好地实现两个世界的异步：并发I/O比线程更有效，比回调更清晰。当然，真正异步比这个更加复杂。真正的框架解决了零拷贝I/O，公平调度，异常处理和许多其他功能。

对于asyncio用户，用协同程序编码比你在这里看到的要简单得多。在上面的代码中，我们第一个原则实现协程，所以你看到回调，任务和future。你甚至看到非阻塞套接字和调用`select`。但是当使用asyncio构建应用程序时，这些都不会出现在您的代码中。你现在已经可以顺利地获取一个URL：

```python
    @asyncio.coroutine
    def fetch(self, url):
        response = yield from self.session.get(url)
        body = yield from response.read()
```

我们回到我们原来的任务：写一个异步的网络爬虫，使用asyncio。

## 协调协同

我们开始描述我们希望抓取器如何工作。现在来实现它与asyncio协程。

我们的抓取器会抓取第一页，解析其链接，并将其添加到队列中。在这之后，它打开网站，同时抓取页面。但是为了限制客户端和服务器上的负载，我们希望运行一些最大数量的工作。每当完成提取页面时，它应该立即从队列中拉下一个链接。我们将过去没有足够的工作周期，所以一些工作必须暂停。但是当一个工作者点击一个新链接的页面时，队列突然编程，任何暂停的工作者都应该醒来并开始破解。最后，我们的程序必须在其工作完成后退出。

想象一下，如果工作是线程。我们将如何表达爬虫的算法？我们可以使用Python标准库中的同步队列。每次将项目放入队列时，队列都会增加其“任务”的计数。完成项目工作后，工作线程调用`task_done`。主线程在`Queue.join`上阻塞，直到每个放入队列的项目被一个`task_done`调用匹配，然后它退出。

协程使用与asyncio队列完全相同的模式！首先我们导入：：

```python
try:
    from asyncio import JoinableQueue as Queue
except ImportError:
    # In Python 3.5, asyncio.JoinableQueue is
    # merged into Queue.
    from asyncio import Queue
```

我们在爬虫类中收集工作的共享状态，并在`crawl`方法中写入主逻辑。 我们在协程上开始`crawl`，并运行asyncio的事件循环，直到`crawl`完成：

```python
loop = asyncio.get_event_loop()

crawler = crawling.Crawler('http://xkcd.com',
                           max_redirect=10)

loop.run_until_complete(crawler.crawl())
```

抓取器以根网址和`max_redirect`开头。它把一对`(URL，max_redirect)`放入队列。

```python
class Crawler:
    def __init__(self, root_url, max_redirect):
        self.max_tasks = 10
        self.max_redirect = max_redirect
        self.q = Queue()
        self.seen_urls = set()

        # aiohttp's ClientSession does connection pooling and
        # HTTP keep-alives for us.
        self.session = aiohttp.ClientSession(loop=loop)

        # Put (URL, max_redirect) in the queue.
        self.q.put((root_url, self.max_redirect))
```

队列中未完成任务的数量现在为1。回到我们的主要脚本，我们启动事件循环和`crawl`方法：

```python
loop.run_until_complete(crawler.crawl())
```

`crawl`协程启动工作。它就像一个主线程：它阻塞在`join`直到所有任务完成，而工作在后台运行。

```python
    @asyncio.coroutine
    def crawl(self):
        """Run the crawler until all work is done."""
        workers = [asyncio.Task(self.work())
                   for _ in range(self.max_tasks)]

        # When all work is done, exit.
        yield from self.q.join()
        for w in workers:
            w.cancel()
```

如果工作是线程，我们可能不希望立即启动它们。为了避免在确定需要之前创建昂贵的线程，线程池通常根据需要增长。 但协同程序很容易，所以我们只是启动允许的最大数量。

有趣的是注意我们如何关闭抓取器。 当加入`join`future解析时，工作任务仍然存活，但被暂停：它们等待更多的URL。因此，主协同程序在退出之前取消它们。否则，当Python解释器关闭并调用所有对象的析构函数时：

```python
ERROR:asyncio:Task was destroyed but it is pending!
```

`cancel`如何工作的？ 生成器具有我们尚未向您展示的功能。 您可以从外部将异常抛出到生成器中：

```python
>>> gen = gen_fn()
>>> gen.send(None)  # Start the generator as usual.
1
>>> gen.throw(Exception('error'))
Traceback (most recent call last):
  File "<input>", line 3, in <module>
  File "<input>", line 2, in gen_fn
Exception: error
```

生成器通过`throw`恢复，但它现在引发异常。如果生成器的调用栈中没有代码捕获它，则异常回到顶部。所以要取消任务的协程：

```python
    # Method of Task class.
    def cancel(self):
        self.coro.throw(CancelledError)
```

无论生成器何时暂停，在某些`yield`语句，它将恢复并引发异常。我们在`step`方法中处理取消：

```python
    # Method of Task class.
    def step(self, future):
        try:
            next_future = self.coro.send(future.result)
        except CancelledError:
            self.cancelled = True
            return
        except StopIteration:
            return

        next_future.add_done_callback(self.step)
```

现在任务知道它被取消。

一旦`crawl`已取消工人，它退出。 事件循环看到协同程序是完整的（我们将看到稍后），它也退出：

```python
loop.run_until_complete(crawler.crawl())
```

`crawl`方法包含了我们的主协同程序必须做的所有函数。它是协程工作，从队列获取URL，并解析它们的新链接。 每个`work`独立地运行工作协同：

```python
    @asyncio.coroutine
    def work(self):
        while True:
            url, max_redirect = yield from self.q.get()

            # Download page and add new links to self.q.
            yield from self.fetch(url, max_redirect)
            self.q.task_done()
```

Python看到这个包含`yield from`语句的代码，并将其编译成一个生成函数。 因此在抓取时，当主协同程序调用`self.work`十次时，它实际上不执行此方法：只啊创建十个生成器对象引用此代码。 它将每个任务包装在一个任务中。任务接收每个future的生成器yield，并且通过在future解析时通过调用具有每个future解析结果的`sending`。 因为生成器具有自己的堆，所以它们独立运行，具有单独的局部变量和指令指针。

工作通过队列与其他工作协调。它等待具有以下内容的URL：

```python
    url, max_redirect = yield from self.q.get()
```

队列里的`get`方法本身是一个协程：它暂停直到有项目被放入队列中，然后恢复并返回。

顺便说一下，工作将在抓取结束时暂停，当主协同程序取消它。 从协同程序的角度来看，最后一个行程循环结束，当`yield from`抛出`CancelledError`。

当工作提取页面时，它将解析链接并将新的链接放入队列，然后调用·task_done·以递减计数器。最终，工作获取已经获取了URL的页面，并且队列中也没有剩余的工作。 因此，对`task_done`的调用将计数器减少为零。等待队列的`join`方法的`crawl`被取消暂停并结束。

我们来解释下为什么队列中的项目是成对的，如：

```python
# URL to fetch, and the number of redirects left.
('http://xkcd.com/353', 10)
```

新URL有十个重定向。获取此特定网址会导致重定向到带有尾部斜杠的新位置。 我们减少剩余的重定向数，并将下一个位置放入队列：

```python
# URL with a trailing slash. Nine redirects left.
('http://xkcd.com/353/', 9)
```

我们使用的`aiohttp包`将遵循重定向默认情况下，并给我们最后的答复。然而，我们告诉它不会在抓取工具中处理重定向，因此它可以合并导致同一目的地的重定向路径：如果我们已经看到这个URL，它在`self.seen_urls`，我们已经开始在这个路径的不同入口点：

![](/images/python/500lines4.png)

抓取器提取“foo”并看到它重定向到“baz”，所以它将“baz”添加到队列和`seen_urls`中。 如果它获取的下一页是“bar”，它也会重定向到“baz”，抓取器不会再次入队“baz”。如果响应是一个页面，而不是重定向，`fetch`解析它的链接，并将新的队列中。

```python
    @asyncio.coroutine
    def fetch(self, url, max_redirect):
        # Handle redirects ourselves.
        response = yield from self.session.get(
            url, allow_redirects=False)

        try:
            if is_redirect(response):
                if max_redirect > 0:
                    next_url = response.headers['location']
                    if next_url in self.seen_urls:
                        # We have been down this path before.
                        return

                    # Remember we have seen this URL.
                    self.seen_urls.add(next_url)

                    # Follow the redirect. One less redirect remains.
                    self.q.put_nowait((next_url, max_redirect - 1))
             else:
                 links = yield from self.parse_links(response)
                 # Python set-logic:
                 for link in links.difference(self.seen_urls):
                    self.q.put_nowait((link, self.max_redirect))
                self.seen_urls.update(links)
        finally:
            # Return connection to pool.
            yield from response.release()
```

如果这是多线程代码，那么它的竞争条件并不好。例如，工作程序检查链接是否在`seen_urls`中，如果不是，则将其放入队列并将其添加到`seen_urls`中。如果它在两个操作之间中断，则另一个工作可以从不同的页面解析相同的链接，观察它不在`seen_urls`中，并且也将其添加到队列。现在同一个链接在队列中两次，导致重复的工作和错误的统计。

然而，协程易受到`yield from`的中断的影响。这是一个关键区别，使得协同代码比多线程代码更不容易发生竞争：多线程代码必须通过抓取锁显式地进入临界区，否则它是可中断的。Python协程在默认情况下是不可中断的，并且只有在它显式产生时才控制。

我们不再需要像在基于回调的程序中一样的fetcher类。该类是回调不足的解决方法：在等待I/O时，它们需要一些地方来存储状态，因为它们的局部变量不会跨调用保留。但是`fetch`协程可以像局部变量那样将其状态存储在局部变量中，则不再需要类。

当`fetch`完成处理服务器响应时，它返回到调用者`work`。工作方法在队列上调用`task_done`，然后从队列中获取要提取的下一个URL。

当`fetch`将新链接放入队列时，它会增加未完成任务的计数，并保持主协程，等待`q.join`，并且停止。但是，如果没有未看见的链接，并且这是队列中的最后一个URL，那么当工作调用`task_done`时，未完成任务的计数降为零。该事件取消暂停连接，主协同程序完成。

协调工作和主协同程序的队列代码如下：

```python
class Queue:
    def __init__(self):
        self._join_future = Future()
        self._unfinished_tasks = 0
        # ... other initialization ...

    def put_nowait(self, item):
        self._unfinished_t---
layout:     post
title:      "500 Lines or Less Chapter 18: A Rejection Sampler ·­Òë"
category:   Translation
tags:       Python Translation
---

* content
{:toc}

## ½éÉÜ

ÔÚ¼ÆËã»ú¿ÆÑ§Óë¹¤³ÌÁìÓò£¬ÎÒÃÇ»áÓöµ½²»ÄÜÓÃ·½³Ì½â¾öµÄÎÊÌâ¡£ÕâÐ©ÎÊÌâÍ¨³£Éæ¼°¸´ÔÓµÄÏµÍ³£¬àÐÔÓµÄÊäÈë»òÁ½Õß¼æ¶øÓÐÖ®¡£ÕâÀïÖ»ÊÇ¼¸¸öÃ»ÓÐ¾«È·½âÎö½â¾ö·½°¸µÄÕæÊµÊÀ½çÎÊÌâµÄÀý×Ó£º

1. ÄãÒÑ¾­´î½¨ÁËÒ»¼Ü·É»úµÄ¼ÆËã»úÄ£ÐÍ£¬²¢Ï£ÍûÈ·¶¨·É»úÔÚ²»Í¬ÌìÆøÌõ¼þÏÂµÄÎÈ¶¨ÐÔ¡£

2. ¸ù¾ÝµØÏÂË®À©É¢Ä£ÐÍ£¬ÄãÏëÈ·¶¨Äâ½¨¹¤³§µÄ»¯Ñ§Æ·Á÷Ê§ÊÇ·ñ»áÓ°Ïì¸½½ü¾ÓÃñµÄ¹©Ë®¡£

3. ÄãÓÐÒ»¸ö¿ÉÒÔ´ÓÏà»úÖÐ²¶»ñàÐÔÓÍ¼ÏñµÄ»úÆ÷ÈË£¬²¢Ï£Íû»Ö¸´ÕâÐ©Í¼ÏñÃè»æµÄ¶ÔÏóµÄÈýÎ¬½á¹¹¡£

4. ²ÉÈ¡ÌØ¶¨¾Ù´ë£¬ÄãÏëÒª¼ÆËãÔÚ¹ú¼ÊÏóÆåÖÐ»ñÊ¤µÄ¿ÉÄÜÐÔ¡£

¼´Ê¹ÕâÐ©ÀàÐÍµÄÎÊÌâ²»ÄÜÍêÈ«½â¾ö£¬ÎÒÃÇ¿ÉÒÔÊ¹ÓÃÃÉÌØ¿¨ÂÞ³éÑù·½·¨µÄ¼¼ÊõÀ´»ñµÃÒ»¸ö½üËÆµÄ½â¾ö·½°¸¡£ÔÚÃÉÌØ¿¨ÂÞ·½·¨ÖÐ£¬¹Ø¼üË¼ÏëÊÇ²ÉÈ¡Ðí¶àÑù±¾£¬È»ºóÔÊÐí¹À¼Æ½â¾ö·½°¸

**Ê²Ã´ÊÇ³éÑù£¿**

³éÑùÒâÎ¶×Å´ÓÄ³ÖÖ¸ÅÂÊ·Ö²¼Éú³ÉËæ»úÖµ¡£ÀýÈç£¬Äã´Ó¹ö¶¯ÁùÃæÌåÄ£¾ß»ñµÃµÄÖµ¾ÍÊÇ¸öÀý×Ó¡£Ï´ÅÆºóÄã´Ó¼×°å¶¥²¿»æÖÆµÄ¿¨Æ¬ÊÇ¸öÀý×Ó¡£·ÉïÚÉäÖÐµÄÎ»ÖÃÒ²ÊÇÒ»¸öÁì×Ó¡£ÕâÐ©Çø±ðÊÇËüÃÇÊÇ´Ó²»Í¬µÄ¸ÅÂÊ·Ö²¼Éú³ÉµÄ¡£¹ö¶¯ÁùÃæÌå·ÖÅäÔÚÁù¸öÖµÖ®¼äÉèÖÃÏàµÈµÄÈ¨ÖØ¡£¿¨ÅÆµÈ·ÖÅä¸ø52¸öÖµ¡£·ÉïÚ°å·ÖÅäÔÚÔ²ÐÎÇøÓò£¨¾¡¹ÜËü¿ÉÄÜ²»ÊÇ¾ùÔÈ·Ö²¼µÄ£¬ÕâÈ¡¾öÓÚ·ÉïÚÍæ¼ÒµÄ¼¼ÄÜ£©¡£

ÎÒÃÇÍ¨³£ÓÃÁ½ÖÖ·½Ê½À´Ê¹ÓÃÑù±¾¡£µÚÒ»¸öÖ»ÊÇ²úÉúÒ»¸öËæ»úÖµ£¬ÒÔ±ãÉÔºóÊ¹ÓÃ£ºÀýÈç£¬ÔÚÆË¿ËµÄµçÄÔÓÎÏ·ÖÐËæ»ú»æÖÆ¿¨Æ¬¡£µÚ¶þÖÖ·½·¨ÊÇ¹À¼Æ¡£ÀýÈç£¬Èç¹ûÄãÔÚ²ÂÏëÄãµÄÅóÓÑÍæµÄ÷»×Ó£¬ÄúÐèÒª¶à´ÎÖÀ÷»×Ó£¬ÒÔ²é¿´Ä³Ð©Êý×ÖÊÇ·ñ±ÈÄúÆÚÍûµÄ¸üÆµ·±¡£»òÕß£¬Äú¿ÉÄÜÖ»ÏëÕÒµ½¿ÉÄÜÐÔµÄ·¶Î§¡£ÌìÆøÊÇÒ»¸öÏàµ±»ìÂÒµÄÏµÍ³£¬ÕâÒâÎ¶×ÅÎÞ·¨×¼È·¼ÆËã·É»úÊÇ·ñÄÜ¹»ÔÚÌØ¶¨ÌìÆøÇé¿öÏÂÉú´æ¡£Ïà·´£¬¿ÉÒÔÔÚ¶àÖÖ²»Í¬µÄÌìÆøÌõ¼þÏÂÄ£Äâ·É»úµÄÐÐÎª£¬ÕâÑù¿ÉÒÔ¿´µ½·É»ú×îÓÐ¿ÉÄÜÊ§°ÜµÄÇé¿ö¡£

**Ê¹ÓÃÊ¾ÀýºÍ¸ÅÂÊ½øÐÐ±à³Ì**

Óë¼ÆËã»ú¿ÆÑ§ÖÐµÄ´ó¶àÊýÓ¦ÓÃÒ»Ñù£¬Äã¿ÉÒÔÔÚ±à³ÌÊ±½øÐÐÉè¼Æ¾ö²ß£¬Ñù±¾ºÍ¸ÅÂÊ½«Ó°Ïì´úÂëµÄÕûÌåÕû½à¶È£¬Ò»ÖÂÐÔºÍÕýÈ·ÐÔ¡£ÔÚ±¾ÕÂÖÐ£¬ÎÒÃÇ½«½éÉÜÈçºÎÔÚµçÄÔÓÎÏ·ÖÐ³éÈ¡Ëæ»úÏîÄ¿µÄ¼òµ¥Ê¾Àý¡£ÌØ±ðÊÇ£¬ÎÒÃÇ½«ÖØµã¹Ø×¢¾ßÌå´¦Àí¸ÅÂÊµÄÉè¼Æ¾ö²ß£¬°üÀ¨³éÑùºÍÆÀ¹À¸ÅÂÊ£¬Ê¹ÓÃ¶ÔÊý£¬ÔÊÐíÖØÏÖÐÔÒÔ¼°½«¾ßÌåÓ¦ÓÃÉú³ÉÑù±¾µÄ¹ý³Ì·Ö¿ªµÄ¹¦ÄÜ¡£

**A Brief Aside About Notation**

We will use mathematical notation like \(p(x)\) to indicate that \(p\) is the probability density function (PDF) or probability mass function (PMF) over values \(x\) of a random variable. A PDF is a continuous function \(p(x)\) such that \(\int_{-\infty}^\infty p(x)\ \mathrm{d}x=1\), whereas a PMF is a discrete function \(p(x)\) such that \(\sum_{x\in \mathbb{Z}} p(x)=1\), where \(\mathbb{Z}\) is the set of all integers.

ÔÚ·ÉïÚ°åµÄÇé¿öÏÂµÄ¸ÅÂÊ·Ö²¼½«ÊÇÁ¬ÐøµÄPDF£¬¶ø·ÉïÚ°åµÄ¸ÅÂÊ·Ö²¼½«ÊÇÀëÉ¢µÄPMF¡£ÔÚÕâÁ½ÖÖÇé¿öÏÂ£¬¶ÔËùÓÐµÄ\(x\)£¬\(p(x) \geq 0\)¸ÅÂÊ±ØÐëÊÇ·Ç¸ºÊý¡£

ÎÒÃÇÒª×öÒ»¸ö¸ÅÂÊ·Ö²¼¡£¸ø¶¨Öµ£¨»òÎ»ÖÃ£©\(x\)£¬ÎÒÃÇÐèÒªÆÀ¹À¸ÃÎ»ÖÃµÄ¸ÅÂÊÃÜ¶È£¨»òÖÊÁ¿£©¡£ÔÚÊýÑ§·ûºÅÖÐ£¬ÎÒÃÇ½«ÆäÐ´Îª\(p(x)\) £¨¸ÅÂÊÃÜ¶ÈÔÚÖµ\(x\)£©¡£

¸ø¶¨PDF»òPMF£¬ÎÒÃÇ¿ÉÄÜ»¹ÒªÒÔÓë·Ö²¼³É±ÈÀýµÄ·½Ê½¶ÔÖµ\(x\)½øÐÐ²ÉÑù£¨ÕâÑùÎÒÃÇ¸üÓÐ¿ÉÄÜÔÚ¸ÅÂÊ½Ï¸ßµÄµØ·½»ñÈ¡Ñù±¾£©¡£ÔÚÊýÑ§·ûºÅÖÐ£¬ÎÒÃÇ½«ÆäÐ´Îª\(x\sim p\)£¬ÒÔ±íÊ¾\(x\)Óë\(p\)³É±ÈÀý¡£

## Sampling Magical Items

×÷ÎªÒ»¸ö¼òµ¥µÄÀý×ÓÀ´Õ¹Ê¾Óë¸ÅÂÊÓÐ¹ØµÄ¸÷ÖÖ±à³ÌÉè¼Æ£¬ÈÃÎÒÃÇÀ´ÏëÏóÒ»ÏÂ£¬ÎÒÃÇÕýÔÚ±àÐ´Ò»¸ö½ÇÉ«°çÑÝÓÎÏ·£¨RPG£©¡£ÎÒÃÇÏëÒªÒ»ÖÖÎª¹ÖÎïËæ»ú¶ªÆúµÄÄ§·¨ÎïÆ·Éú³É½±½ðÍ³¼ÆµÄ·½·¨¡£ÎÒÃÇÏëÒªµÄÏîÄ¿µÄ×î´ó½±½ðÊÇ+5£¬¶ø½Ï¸ßµÄ½±½ð±È½ÏµÍµÄ½±½ð²»Ì«¿ÉÄÜ¡£Èç¹û\(B\)ÊÇ½±½ðÖµµÄËæ»ú±äÁ¿£¬Ôò£º

\[ p(B=\mathrm{+1}) = 0.55\\ p(B=\mathrm{+2}) = 0.25\\ p(B=\mathrm{+3}) = 0.12\\ p(B=\mathrm{+4}) = 0.06\\ p(B=\mathrm{+5}) = 0.02 \]

ÎÒÃÇ»¹¿ÉÒÔÖ¸³ö£¬½±½ðÓ¦¸Ã·ÖÅäÔÚÁù¸öÍ³¼Æ£¨dexterity, constitution, strength, intelligence, wisdom, and charisma£©¡£ËùÒÔ£¬¾ßÓÐ+5¼ÓÖµµÄÏîÄ¿¿ÉÒÔ½«ÕâÐ©µã·Ö²¼ÔÚ²»Í¬µÄÍ³¼ÆÊý¾Ý£¨ÀýÈç+2 wisdom and +3 intelligence£©ÖÐ£¬»òÕß¼¯ÖÐÔÚµ¥¸öÍ³¼ÆÊý¾Ý£¨ÀýÈç+5 charisma£©¡£

ÎÒÃÇÈçºÎ´ÓÕâ¸ö·Ö²¼ÖÐËæ»ú³éÑù£¿×î¼òµ¥µÄ·½·¨ÊÇ¶ÔÕûÌåÏîÄ¿½±½ð½øÐÐ³éÑù£¬È»ºó¶Ô½±½ð·Ö²¼ÔÚÍ³¼ÆÊý¾ÝÖÐµÄ·½Ê½½øÐÐ³éÑù¡£½±½ðµÄ¸ÅÂÊ·Ö²¼ºÍ·ÖÅä·½Ê½¶¼ÊÇ¶àÏîÊ½·Ö²¼µÄÊµÀý¡£

## ¶àÏî·Ö²¼

µ±ÄãÓÐ¶à¸ö¿ÉÄÜµÄ½á¹û²¢ÇÒÏëÒª±íÊ¾Ã¿¸ö½á¹û·¢ÉúµÄ¸ÅÂÊ£¬Ê¹ÓÃ¶àÏî·Ö²¼¡£ÓÃÓÚ½âÊÍ¶àÏîÊ½·Ö²¼µÄ¾­µäÀý×ÓÊÇÇòºÍºÐ×Ó¡£ÄãÓÐÒ»¸ö´øÓÐ²»Í¬ÑÕÉ«µÄÇòµÄºÐ×Ó£¨ÀýÈç£¬30£¥µÄºìÉ«£¬20£¥µÄÀ¶É«ºÍ50£¥µÄÂÌÉ«£©¡£ÄãÈ¡³öÒ»¸öÇò£¬¼ÇÂ¼ËüµÄÑÕÉ«£¬°ÑËü·Å»ØºÐ×ÓÀï£¬È»ºóÔÙÖØ¸´Ò»´Î¡£ÕâÖÖÇé¿öÏÂ£¬½á¹û¶ÔÓ¦ÓÚÌØ¶¨ÑÕÉ«µÄÇò£¬²¢ÇÒÃ¿¸ö½á¹ûµÄ¸ÅÂÊ¶ÔÓ¦ÓÚ¸ÃÑÕÉ«µÄÇòµÄ±ÈÀý£¨ÀýÈçÀ¶É«ÇòµÄ½á¹û£¬¸ÅÂÊÎª\(p(\mathrm{blue})=0.20\)¡£È»ºó£¬¶àÏî·Ö²¼ÓÃÓÚÃèÊöµ±»æÖÆ¶à¸öÇòÊ±£¨ÀýÈç£¬Á½¸öÂÌÉ«ºÍÒ»¸öÀ¶É«£©Ê±¿ÉÄÜµÄ½á¹û×éºÏ¡£

±¾½ÚÖÐµÄ´úÂëÎ»ÓÚÎÄ¼þ`multinomial.py`ÖÐ¡£

**`MultinomialDistribution `Àà`**

Ò»°ãÀ´Ëµ£¬·Ö²¼ÓÐÁ½ÖÖ£ºÎÒÃÇÏ£Íû´Ó¸Ã·Ö²¼ÖÐ½øÐÐ³éÑù£¬ÎÒÃÇÐèÒªÆÀ¹À¸Ã·Ö²¼µÄPMF»òPDFÏÂÑù±¾µÄ¸ÅÂÊ¡£ËäÈ»Ö´ÐÐÕâÁ½¸ö¹¦ÄÜËùÐèµÄÊµ¼Ê¼ÆËãÊÇÏàµ±²»Í¬µÄ£¬µ«ËüÃÇÒÀÀµÓÚÒ»¸ö¹²Í¬µÄÐÅÏ¢£º·Ö²¼µÄ²ÎÊýÊÇÊ²Ã´¡£ÔÚ¶àÏîÊ½·Ö²¼µÄÇé¿öÏÂ£¬²ÎÊýÊÇÊÂ¼þ¸ÅÂÊ£¬\(p\)£¨¶ÔÓ¦ÓÚÉÏÊöÀý×ÓÖÐµÄ²»Í¬²ÊÉ«ÇòµÄ±ÈÀý£©¡£

×î¼òµ¥µÄ½â¾ö·½°¸ÊÇ´´½¨Á½¸öº¯Êý£¬Á½¸öº¯Êý¶¼²ÉÓÃÏàÍ¬µÄ²ÎÊý£¬µ«ÊÇÏà»¥¶ÀÁ¢¡£ÎÒÍ¨³£»áÑ¡ÔñÊ¹ÓÃÒ»¸öÀàÀ´±íÊ¾ÎÒµÄ·Ö²¼¡£ÕâÑù×öÓÐ¼¸¸öºÃ´¦£º

1. ´´½¨ÀàÊ±Ö»ÐèÒª´«Èë²ÎÊýÒ»´Î¡£

2. »¹ÓÐÒ»Ð©¶îÍâµÄÊôÐÔ£¬ÎÒÃÇ¿ÉÄÜÏëÖªµÀÒ»¸ö·Ö²¼£ºÆ½¾ùÖµ£¬·½²î£¬µ¼ÊýµÈ¡£Ò»µ©ÎÒÃÇÓÐÔÚ¹²Í¬µÄ¶ÔÏóÉÏ²Ù×÷µÄº¯Êý£¬ÉõÖÁ¸ü·½±ãÊ¹ÓÃÒ»¸öÀà£¬¶ø²»ÊÇ´«µÝÏàÍ¬µÄ²ÎÊý¸øºÜ¶à²»Í¬µÄ¹¦ÄÜ¡£

3. ¼ì²é²ÎÊýÖµÊÇ·ñÓÐÐ§Í¨³£ÊÇÒ»¸öºÃÖ÷Òâ£¨ÀýÈç£¬ÔÚ¶àÏîÊ½·Ö²¼µÄÇé¿öÏÂ£¬ÊÂ¼þ¸ÅÂÊµÄÏòÁ¿\(p\) Ó¦¸ÃµÈÓÚ1£©¡£ÔÚÀàµÄ¹¹Ôìº¯ÊýÖÐÖ´ÐÐ´Ë¼ì²éÒ»´Î¸üÓÐÐ§£¬¶ø²»ÊÇÃ¿´Îµ÷ÓÃÆäÖÐÒ»¸öº¯Êý¡£

4. ÓÐÊ±¼ÆËãPMF»òPDFÉæ¼°¼ÆËã³£ÊýÖµ¡£Ê¹ÓÃÀà£¬ÎÒÃÇ¿ÉÒÔÔÚ¹¹Ôìº¯ÊýÖÐÔ¤ÏÈ¼ÆËãÕâÐ©³£Á¿£¬¶ø²»±ØÃ¿´Îµ÷ÓÃPMF»òPDFº¯ÊýÊ±¼ÆËãËüÃÇ¡£

Êµ¼Ê²Ù×÷ÖÐ£¬ÕâÊÇÍ³¼ÆÊý¾Ý°üµÄ¹¤×÷Á¿£¬°üÀ¨SciPy£¬ËüÃÇÎ»ÓÚ`scipy.stats`Ä£¿éÖÐ¡£È»¶ø£¬ËäÈ»ÎÒÃÇÊ¹ÓÃÆäËûSciPyº¯Êý£¬µ«ÊÇÎªÁËËµÃ÷Æð¼û£¬ÎÒÃÇÃ»ÓÐÊ¹ÓÃ°üÀïÃæµÄ¸ÅÂÊ·Ö²¼£¬ÒòÎªSciPyÄ¿Ç°Ã»ÓÐ¶àÏîÊ½·Ö²¼¡£

ÕâÊÇÀàµÄ¹¹Ôìº¯Êý´úÂë£º

```python
import numpy as np

class MultinomialDistribution(object):

    def __init__(self, p, rso=np.random):
        """Initialize the multinomial random variable.

        Parameters
        ----------
        p: numpy array of length `k`
            The event probabilities
        rso: numpy RandomState object (default: None)
            The random number generator

        """

        # Check that the probabilities sum to 1. If they don't, then
        # something is wrong! We use `np.isclose` rather than checking
        # for exact equality because in many cases, we won't have
        # exact equality due to floating-point error.
        if not np.isclose(np.sum(p), 1.0):
            raise ValueError("event probabilities do not sum to 1")

        # Store the parameters that were passed in
        self.p = p
        self.rso = rso

        # Precompute log probabilities, for use by the log-PMF, for
        # each element of `self.p` (the function `np.log` operates
        # elementwise over NumPy arrays, as well as on scalars.)
        self.logp = np.log(self.p)
```

¸ÃÀà½«ÊÂ¼þ¸ÅÂÊ×÷Îª²ÎÊý£¬ \(p\)ºÍ±äÁ¿`rso`¡£Ê×ÏÈ£¬¹¹Ôìº¯Êý¼ì²é²ÎÊýÊÇ·ñÓÐÐ§£º¼´`p`×ÜºÍÎª1£¬²¢´æ´¢£¬Ê¹ÓÃÊÂ¼þ¸ÅÂÊÀ´¼ÆËãÊÂ¼þÈÕÖ¾¸ÅÂÊ¡£ÎÒÃÇÉÔºó½«ÓÃ`rso`²úÉúËæ»úÊý¡£

ÔÚÎÒÃÇ½øÈëÆäÓà²¿·ÖÖ®Ç°£¬ÏÈÀ´¿´¿´Óë¹¹Ôìº¯ÊýÏà¹ØµÄÁ½µã¡£ 

**Descriptive versus Mathematic Variable Names
**

Í¨³££¬¹ÄÀø³ÌÐòÔ±Ê¹ÓÃÃèÊöÐÔ±äÁ¿Ãû³Æ£ºÀýÈç£¬Ê¹ÓÃÃû³Æ`independent_variable`ºÍ`dependent_variable`¶ø²»ÊÇ`x`ºÍ`y`¡£Ò»¸ö±ê×¼µÄ¾­Ñé·¨ÊÇÓÀÔ¶²»ÒªÊ¹ÓÃÖ»ÓÐÒ»¸ö»òÁ½¸ö×Ö·ûµÄ±äÁ¿Ãû¡£µ«ÊÇ£¬£¬ÔÚ`MultinomialDistribution`ÀàµÄ¹¹Ôìº¯ÊýÖÐ£¬ÎÒÃÇÊ¹ÓÃ`p`µÄ±äÁ¿Ãû£¬ÕâÎ¥·´ÁËµäÐÍµÄÃüÃûÔ¼¶¨¡£

ËäÈ»ÕâÑùµÄÃüÃûÔ¼¶¨Ó¦¸ÃÊÊÓÃÓÚ¼¸ºõÃ¿¸öÁìÓò£¬µ«ÓÐÒ»¸öÀýÍâ£ºÊýÑ§¡£±àÂëÊýÑ§·½³ÌÊ½µÄÄÑµãÔÚÓÚ£¬ÕâÐ©·½³ÌÍ¨³£¾ßÓÐ½öÎªµ¥¸ö×ÖÄ¸µÄ±äÁ¿Ãû³Æ£º \(x\), \(y\), \(\alpha\)µÈµÈ¡£Òò´Ë£¬Èç¹ûÄãÖ±½ÓÓÃ½ø´úÂëÀï£¬×î¼òµ¥µÄ±äÁ¿Ãû³Æ½«ÊÇ`x`£¬`y`ºÍ`alpha`¡£ÏÔÈ»£¬ÕâÐ©²»ÊÇ×îÓÐËµ·þÁ¦µÄ±äÁ¿Ãû³Æ£¨`x`²»´«´ïºÜ¶àÐÅÏ¢£©£¬µ«ÊÇ¾ßÓÐ¸ü¶àÃèÊöÐÔ±äÁ¿Ãû³ÆÒ²¿ÉÄÜÊ¹´úÂëºÍ·½³ÌÖ®¼äµÄÇÐ»»±äµÃ¸ü¼ÓÀ§ÄÑ¡£

ÎÒÈÏÎªµ±Äã±àÐ´Ö±½ÓÊµÏÖ·½³ÌµÄ´úÂëÊ±£¬Ó¦¸ÃÊ¹ÓÃÓë·½³ÌÊ½ÏàÍ¬µÄ±äÁ¿Ãû¡£ÕâÊ¹µÃºÜÈÝÒ×¿´³ö´úÂëµÄÄÄÐ©²¿·Ö¶ÔÓ¦ÊµÏÖ·½³ÌµÄÄÄÐ©²¿·Ö¡£Õâµ±È»¿ÉÒÔÊ¹´úÂë¸üÈÝÒ×±»¹ÂÁ¢µØÀí½â£¬ËùÒÔÌØ±ðÖØÒªµÄÊÇ£¬Ôö¼Ó×¢ÊÍ¸üºÃµÄ½âÊÍ£¬¸÷ÖÖ¼ÆËãµÄÄ¿±êÊÇÊ²Ã´¡£Èç¹û·½³ÌÊ½ÔÚÑ§ÊõÂÛÎÄÖÐÁÐ³ö£¬ÔòÓ¦¸ÃÒýÓÃ·½³ÌºÅ£¬ÒÔ±ãÈÝÒ×µØ²éÕÒ¡£

** Importing NumPy**

ÎÒÃÇ½«`numpy`Ä£¿éµ¼ÈëÎª`np`¡£ÕâÊÇÊýÖµ¼ÆËãÁìÓòµÄ±ê×¼Êµ¼ù£¬ÒòÎª`NumPy`Ìá¹©ÁË´óÁ¿ÓÐÓÃµÄ¹¦ÄÜ£¬ÆäÖÐÐí¶à¹¦ÄÜÉõÖÁ¿ÉÒÔÔÚµ¥¸öÎÄ¼þÖÐÊ¹ÓÃ¡£ÔÚ±¾ÕÂµÄ¼òµ¥Àý×ÓÖÐ£¬ÎÒÃÇÖ»Ê¹ÓÃ11¸ö`NumPy`º¯Êý£¬µ«¿ÉÒÔ¸ü¶à£ºÔÚÕû¸öÏîÄ¿ÖÐÍ¨³£´óÔ¼Ê¹ÓÃ40¸ö²»Í¬µÄ`NumPy`º¯Êý;

ÓÐ¼¸¸ö¿ÉÒÔµ¼Èë`NumPy`µÄÑ¡Ïî¡£ÎÒÃÇ¿ÉÒÔÊ¹ÓÃ`from numpy import *`£¬µ«ÊÇÍ¨³£ÕâÖÖ·ç¸ñ²»ºÃ£¬ÒòÎªËüºÜÄÑÈ·¶¨º¯ÊýµÄÀ´Ô´¡£ÎÒÃÇ¿ÉÒÔÊ¹ÓÃ`from numpy import array, log, ...`·Ö±ðµ¼ÈëÕâÐ©º¯Êý¡£ÎÒÃÇÖ»ÓÃ`import numpy`£¬Í¨³£»áµ¼ÖÂ´úÂëÄÑÒÔÔÄ¶Á¡£ÒÔÏÂÁ½¸öÊ¾ÀýºÜÄÑÔÄ¶Á£¬µ«Ê¹ÓÃ`np`¶ø²»ÊÇ`numpy`µÄÀý×Ó¸üÇåÎú£º

```python
>>> numpy.sqrt(numpy.sum(numpy.dot(numpy.array(a), numpy.array(b))))
>>> np.sqrt(np.sum(np.dot(np.array(a), np.array(b))))
```

**Sampling from a Multinomial Distribution**

´Ó¶àÏî·Ö²¼»ñÈ¡Ñù±¾Êµ¼ÊÉÏÊÇÏàµ±¼òµ¥µÄ£¬ÒòÎª`NumPy`ÎªÎÒÃÇÌá¹©ÁËÒ»¸ö¹¦ÄÜ£º`np.random.multinomial`¡£

¾¡¹ÜÕâ¸öº¯ÊýÒÑ¾­ÓÐÁË£¬µ«ÎÒÃÇ¿ÉÒÔÎ§ÈÆËü×öÒ»Ð©Éè¼Æ¡£

**Seeding the Random Number Generator**

¼´Ê¹ÎÒÃÇÏëÒª»æÖÆÒ»¸öËæ»úÑù±¾£¬ÓÐÊ±Ï£Íû½á¹ûÊÇ¿ÉÖØÏÖµÄ£º¼´Ê¹Êý×Ö¿´ÆðÀ´ÊÇËæ»úµÄ£¬Èç¹ûÎÒÃÇÔÙ´ÎÔËÐÐ³ÌÐò£¬ÎÒÃÇ¿ÉÄÜÏ£ÍûÊ¹ÓÃÏàÍ¬µÄ¡°Ëæ»ú¡±Êý×ÖÐòÁÐ ¡£

ÎªÁËÔÊÐí²úÉúÕâÑùµÄ¡°¿ÉÖØ¸´Ëæ»ú¡±Êý×Ö£¬ÎÒÃÇÐèÒª¸æËßÎÒÃÇµÄ²ÉÑùº¯ÊýÈçºÎÉú³ÉËæ»úÊý¡£ÎÒÃÇ¿ÉÒÔÍ¨¹ýÊ¹ÓÃNumPy`RandomState`¶ÔÏóÀ´ÊµÏÖ£¬Æä±¾ÖÊÉÏÊÇ¿ÉÒÔ´«µÝµÄËæ»úÊýÉú³ÉÆ÷¶ÔÏó¡£Ëü¾ßÓÐ´ó¶àÊýÓë`np.random`ÏàÍ¬µÄ¹¦ÄÜ; ²»Í¬µÄÊÇÎÒÃÇ¿ÉÒÔ¿ØÖÆËæ»úÊý¡£ÎÒÃÇ´´½¨ÈçÏÂ£º

```python 
>>> import numpy as np
>>> rso = np.random.RandomState(230489)
```

´«µÝ¸ø`RandomState`¹¹Ôìº¯ÊýµÄÊý×ÖÊÇËæ»úÊýÉú³ÉÆ÷µÄÖÖ×Ó¡£Ö»ÒªÎÒÃÇÊ¹ÓÃÏàÍ¬µÄÖÖ×ÓÊµÀý»¯Ëü£¬`RandomState`¶ÔÏó½«ÒÔÏàÍ¬µÄË³ÐòÉú³ÉÏàÍ¬µÄ¡°Ëæ»ú¡±Êý×Ö£¬´Ó¶øÈ·±£¿É¸´ÖÆÐÔ£º

```python
>>> rso.rand()
0.5356709186237074
>>> rso.rand()
0.6190581888276206
>>> rso.rand()
0.23143573416770336
>>> rso.seed(230489)
>>> rso.rand()
0.5356709186237074
>>> rso.rand()
0.6190581888276206
```

Ö®Ç°£¬ÎÒÃÇ¿´µ½¹¹Ôìº¯ÊýÒªÇó`rso`µÄ²ÎÊý¡£Õâ¸ö`rso`±äÁ¿ÊÇÒÑ¾­³õÊ¼»¯µÄ`RandomState`¶ÔÏó¡£°Ñ`RandomState`¶ÔÏó×÷ÎªÒ»¸ö¿ÉÑ¡²ÎÊý£ºÎÒÏ£ÍûÓÐÊ±Ñ¡ÔñÊ¹ÓÃËü£¨Èç¹ûÎÒÖ»ÊÇÊ¹ÓÃ`np.random`Ä£¿é£©¡£

ËùÒÔÈç¹ûÃ»ÓÐ`rso`±äÁ¿£¬ÄÇÃ´¹¹Ôìº¯ÊýÄ¬ÈÏÎª`np.random.multinomial`¡£·ñÔò£¬ËüÊ¹ÓÃ`RandomState`¶ÔÏó±¾ÉíµÄ¶àÏîÈ¡ÑùÆ÷¡£

**Ê²Ã´ÊÇ²ÎÊý£¿**

Ò»µ©ÎÒÃÇ¾ö¶¨ÊÇ·ñÊ¹ÓÃ`np.random.multinomial`»ò`rso.multinomial`£¬³éÑùÖ»ÊÇµ÷ÓÃÏàÓ¦µÄº¯Êý¡£È»¶ø»¹Òª£ºÊ²Ã´ÊÇÒ»¸ö²ÎÊý£¿

ÔçÐ©Ê±ºò£¬ÎÒËµ½á¹û¸ÅÂÊ\(p\)ÊÇ¶àÏîÊ½·Ö²¼µÄ²ÎÊý¡£µ«ÊÇ£¬¸ù¾ÝÄãµÄÒªÇó£¬ÊÂ¼þÊýÁ¿\(n\)Ò²¿ÉÒÔÊÇ¶àÏîÊ½·Ö²¼µÄ²ÎÊý¡£ÄÇÃ´ÎªÊ²Ã´ÎÒÃÇ²»°Ñ\(n\)×÷Îª²ÎÊýÄØ£¿

Õâ¸öÎÊÌâËäÈ»ÔÚ¶àÏî·Ö²¼·½Ãæ±È½Ï¾ßÌå£¬µ«ÔÚ´¦Àí¸ÅÂÊ·Ö²¼Ê±Êµ¼Ê³öÏÖµÃºÜ¶à£¬´ð°¸È¡¾öÓÚÓÃÀý¡£¶ÔÓÚÒ»¸ö¶àÏîÊ½£¬Äã¿ÉÒÔ¼ÙÉèÊÂ¼þµÄÊýÁ¿×ÜÊÇÏàÍ¬µÄÂð£¿Èç¹ûÊÇÕâÑù£¬ÄÇÃ´¿ÉÄÜÒÔ\(n\)×÷Îª²ÎÊý´«µÝ¸ø¹¹Ôìº¯Êý¡£Èç¹ûÃ»ÓÐ£¬ÔòÔÚ¶ÔÏó´´½¨Ê±ÒªÖ¸¶¨\(n\)ÓÐ·Ç³£ÑÏ¸ñµÄÏÞÖÆ£¬ÉõÖÁ¿ÉÄÜÐèÒªÄúÔÚÃ¿´ÎÐèÒª»æÖÆÑù±¾Ê±´´½¨Ò»¸öÐÂµÄ·Ö²¼¶ÔÏó£¡

ÎÒÍ¨³£²»Ï²»¶±»ÎÒµÄ´úÂëÏÞÖÆ£¬Òò´ËÑ¡Ôñ`n`×÷Îª`samle`º¯ÊýµÄ²ÎÊý£¬¶ø²»ÊÇÊ¹Æä³ÉÎª¹¹Ôìº¯ÊýµÄ²ÎÊý¡£Ò»¸ö¿ÉÑ¡µÄ½â¾ö·½°¸ÊÇ`n`×÷Îª¹¹Ôìº¯ÊýµÄ²ÎÊý£¬¶øÇÒ»¹°üÀ¨ÔÊÐí¸ü¸Ä`n`ÖµµÄ·½·¨£¬¶ø²»±Ø´´½¨Ò»¸öÈ«ÐÂµÄ¶ÔÏó¡£µ«ÊÇ£¬ÎªÁËÎÒÃÇµÄÄ¿µÄ£¬Õâ¸ö½â¾ö·½°¸ÊÇÔÝÊ±µÄ£¬ËùÒÔÎÒÃÇ¼á³Ö½«Æä×÷Îª`sample`µÄ²ÎÊý£º

```python
def sample(self, n):
    """Samples draws of `n` events from a multinomial distribution with
    outcome probabilities `self.p`.

    Parameters
    ----------
    n: integer
        The number of total events

    Returns
    -------
    numpy array of length `k`
        The sampled number of occurrences for each outcome

    """
    x = self.rso.multinomial(n, self.p)
    return x
```

## Evaluating the Multinomial PMF

ËäÈ»ÎÒÃÇ²¢Ã»ÓÐÃ÷È·µØ¼ÆËã³öÎÒÃÇÉú³ÉÎïÆ·µÄ¸ÅÂÊ£¬µ«ÊÇÐ´Ò»¸ö¿ÉÒÔ¼ÆËã·Ö²¼µÄ¸ÅÂÊÖÊÁ¿º¯Êý£¨PMF£©»ò¸ÅÂÊÃÜ¶Èº¯Êý£¨PDF£©µÄº¯ÊýÊÇÓÐÓÃµÄ¡£ÎªÊ²Ã´£¿

Ò»¸öÔ­ÒòÊÇÎÒÃÇ¿ÉÒÔÊ¹ÓÃËü½øÐÐ²âÊÔ£ºÈç¹ûÎÒÃÇ²ÉÓÃ³éÑùº¯Êý³éÈ¡ºÜ¶àÑù±¾£¬ÄÇÃ´ËüÃÇÓ¦¸Ã½üËÆÓÚPDF»òPMF¡£Èç¹ûÔÚºÜ¶àÑù±¾Ö®ºó£¬½üËÆÖµ²îÓÐÃ÷ÏÔ´íÎó£¬Ôò·¢ÏÖ´úÂëÖÐÓÐ´íÎó¡£

**¶àÏîÊ½PMF·½³Ì**

ÐÎÊ½ÉÏ£¬¶àÏîÊ½·Ö²¼¾ßÓÐÒÔÏÂµÈÊ½£º

\[ p(\mathbf{x}; \mathbf{p}) = \frac{(\sum_{i=1}^k x_i)!}{x_1!\cdots{}x_k!}p_1^{x_1}\cdots{}p_k^{x_k} \]

where \(\mathbf{x}=[x_1, \ldots{}, x_k]\) is a vector of length \(k\) specifying the number of times each event happened, and \(\mathbf{p}=[p_1, \ldots{}, p_k]\) is a vector specifying the probability of each event occurring. As mentioned above, the event probabilities \(\mathbf{p}\) are the parameters of the distribution.

ÉÏÊöµÈÊ½ÖÐµÄ½×³ËÊµ¼ÊÉÏÊÇÙ¤Âêº¯ÊýµÄÌØÊâº¯Êý\(\Gamma\)±íÊ¾¡£ µ±ÎÒÃÇ±àÐ´´úÂëÊ±£¬Ê¹ÓÃgammaº¯Êý¶ø²»ÊÇfactorial½«¸ü·½±ãºÍ¸üÓÐÐ§£¬ËùÒÔÎÒÃÇ½«Ê¹ÓÃ\(\Gamma\)ÖØÐ´·½³Ì£º

\[ p(\mathbf{x}; \mathbf{p}) = \frac{\Gamma((\sum_{i=1}^k x_i)+1)}{\Gamma(x_1+1)\cdots{}\Gamma(x_k+1)}p_1^{x_1}\cdots{}p_k^{x_k} \]

**Working with Log Values**

ÔÚÊµÏÖÉÏÊö·½³ÌÖ®Ç°£¬ÎÒÏëÇ¿µ÷±àÐ´´úÂëÊ±×îÖØÒªµÄÉè¼Æ¾ö²ßÖ®Ò»£ºÊ¹ÓÃlogÖµ¡£ÕâÒâÎ¶×Å£¬¶ø²»ÊÇÖ±½ÓÊ¹ÓÃ¸ÅÂÊ \(p(x)\)£¬ÎÒÃÇÓ¦¸ÃÊ¹ÓÃlog-probability£¬\(\log{p(x)}\)¡£Õâ¿ÉÄÜµ¼ÖÂÏÂÒç´íÎó¡£

ÎªÁË´ïµ½ÕâÒ»µã£¬¿¼ÂÇ¸ÅÂÊ±ØÐë½éÓÚ0ºÍ1Ö®¼ä£¨°üº¬£©¡£NumPyÓÐÒ»¸öÓÐÓÃµÄº¯Êý£¬`finfo`£¬½«¸æËßÎÒÃÇÏµÍ³µÄ¸¡µãÖµµÄ¼«ÏÞ¡£ÀýÈç£¬ÔÚ64Î»»úÆ÷ÉÏ£¬ÎÒÃÇ¿´µ½×îÐ¡µÄÕýÊýÊÇ£º

```python
>>> import numpy as np
>>> np.finfo(float).tiny
2.2250738585072014e-308
```

ËäÈ»Õâ¿´ÆðÀ´ºÜÐ¡£¬µ«ÊÇÓöµ½´ó¸ÅÂÊÉõÖÁ¸üÐ¡µÄ¸ÅÂÊ²¢²»º±¼û¡£ ´ËÍâ£¬ÆÕ±éµÄ²Ù×÷À´Ôö¼Ó¸ÅÂÊ£¬µ«ÊÇÈç¹ûÎÒÃÇ³¢ÊÔÒÔ·Ç³£Ð¡µÄ¸ÅÂÊÀ´×öµ½ÕâÒ»µã£¬ÎÒÃÇÓöµ½ÏÂÒçÎÊÌâ£º

```python
>>> tiny = np.finfo(float).tiny
>>> # if we multiply numbers that are too small, we lose all precision
>>> tiny * tiny
0.0
```

µ«ÊÇ£¬Ê¹ÓÃlog¿ÉÒÔ°ïÖú½ã½ãÕâ¸öÎÊÌâ£¬ÒòÎªÎÒÃÇ¿ÉÒÔÓÃ¶ÔÊý±íÊ¾¸ü¹ã·ºµÄÊý×Ö¡£ÈÕÖ¾ÖµµÄ·¶Î§´Ó \(-\infty\)µ½Áã¡£Êµ¼ÊÉÏ£¬ËüÃÇµÄ·¶Î§´Ó`finfo`·µ»ØµÄ`min`ÖµÎªÁã¡£×îÐ¡ÖµÔ¶Ð¡ÓÚÎ¢Ð¡ÖµµÄlog£º

```python
>>> # this is our lower bound normally
>>> np.log(tiny)
-708.39641853226408
>>> # this is our lower bound when using logs
>>> np.finfo(float).min
-1.7976931348623157e+308
```

Òò´Ë£¬Í¨¹ýlogÖµ£¬ÎÒÃÇ¿ÉÒÔ´ó´óÀ©Õ¹¿É±íÊ¾Êý×ÖµÄ·¶Î§¡£´ËÍâ£¬¿ÉÒÔÍ¨¹ýÊ¹ÓÃ¼Ó·¨À´Ö´ÐÐÓëlogµÄ³Ë·¨£¬ÒòÎª\(\log(x\cdot{}y) = \log(x) + \log(y)\)¡£Èç¹ûÎÒÃÇÓÃlog½øÐÐÉÏÃæµÄ³Ë·¨£¬¾Í²»ÓÃµ£ÐÄÓÉÓÚÏÂÒçÔì³ÉµÄ¾«¶ÈËðÊ§£º

```python
>>> # the result of multiplying small probabilities
>>> np.log(tiny * tiny)
-inf
>>> # the result of adding small log probabilities
>>> np.log(tiny) + np.log(tiny)
-1416.7928370645282
```

Èç¹ûÎÒÃÇÐèÒª´Ó¶ÔÊýÖÐµ¼³öÊý×Ö£¨ÀýÈç£¬Ìí¼Ó¸ÅÂÊ£¬¶ø²»ÊÇ³ËÒÔËüÃÇ£©£¬ÄÇÃ´ÎÒÃÇ»Øµ½ÏÂÒç£º

```python
>>> tiny*tiny
0.0
>>> np.exp(np.log(tiny) + np.log(tiny))
0.0
```

È»¶ø£¬ÓÃlog¼ÆËã¿ÉÒÔÊ¡ºÜ¶à·³ÄÕ¡£Èç¹ûÎÒÃÇÐèÒª»Øµ½Ô­À´µÄÊý×Ö£¬¿ÉÄÜ»á±»ÆÈÊ§È¥¾«¶È£¬µ«ÊÇÖÁÉÙ±£Áô¸ÅÂÊµÄÒ»Ð©ÐÅÏ¢£¬ÀýÈç×ãÒÔ±È½ÏËüÃÇ£¬·ñÔò»á¶ªÊ§¡£

**Writing the PMF Code**

ÏÖÔÚÎÒÃÇÒÑ¾­¿´µ½Ê¹ÓÃlogµÄÖØÒªÐÔ£¬ÎÒÃÇÐ´¸öº¯ÊýÀ´¼ÆËãlog-PMF£º

```python
def log_pmf(self, x):
    """Evaluates the log-probability mass function (log-PMF) of a
    multinomial with outcome probabilities `self.p` for a draw `x`.

    Parameters
    ----------
    x: numpy array of length `k`
        The number of occurrences of each outcome

    Returns
    -------
    The evaluated log-PMF for draw `x`

    """
    # Get the total number of events
    n = np.sum(x)

    # equivalent to log(n!)
    log_n_factorial = gammaln(n + 1)
    # equivalent to log(x1! * ... * xk!)
    sum_log_xi_factorial = np.sum(gammaln(x + 1))

    # If one of the values of self.p is 0, then the corresponding
    # value of self.logp will be -inf. If the corresponding value
    # of x is 0, then multiplying them together will give nan, but
    # we want it to just be 0.
    log_pi_xi = self.logp * x
    log_pi_xi[x == 0] = 0
    # equivalent to log(p1^x1 * ... * pk^xk)
    sum_log_pi_xi = np.sum(log_pi_xi)

    # Put it all together
    log_pmf = log_n_factorial - sum_log_xi_factorial + sum_log_pi_xi
    return log_pmf
```

大多数情况下，这是多项式PMF的直接实现。`gammaln`函数来自`scipy.special`，并计算`log-gamma`函数\(\log{\Gamma(x)}\)。 如上所述，使用gamma函数比factorial函数更方便; 这是因为SciPy提供了log-gamma函数，而不是log-factorial函数。我们可以自己计算一个对数因子，类似于

```python
log_n_factorial = np.sum(np.log(np.arange(1, n + 1)))
sum_log_xi_factorial = np.sum([np.sum(np.log(np.arange(1, i + 1))) for i in x])
```

如果我们使用已经建立在SciPy中的gamma函数，更容易理解和编码，并且计算效率更高。

我们需要处理一个边缘案例：当一个概率为零时。When \(p_i=0\), then \(\log{p_i}=-\infty\)。这将是正常的，除了以下的无穷大乘以零时：

```python
>>> # it's fine to multiply infinity by integers...
>>> -np.inf * 2.0
-inf
>>> # ...but things break when we try to multiply by zero
>>> -np.inf * 0.0
nan
```

`nan`指的是“不是一个数字”，因为大多与`nan`的计算生成`nan`。 所以，如果我们不处理\(p_i=0\) 和\(x_i=0\) 的情况，我们会得到一个`nan`。将其与其他数字相加，产生另一个`nan`，这是无效的。我们特别检查\(x_i=0\) 的情况，并将生成的\(x_i\cdot{}\log(p_i)\)设置为零。

让我们回顾一下log。即使我们真的只需要PMF，而不是log-PMF，最好先用日志计算它，然后如果我们需要计算exponentiate：

```python
def pmf(self, x):
    """Evaluates the probability mass function (PMF) of a multinomial
    with outcome probabilities `self.p` for a draw `x`.

    Parameters
    ----------
    x: numpy array of length `k`
        The number of occurrences of each outcome

    Returns
    -------
    The evaluated PMF for draw `x`

    """
    pmf = np.exp(self.log_pmf(x))
    return pmf
```

为了进一步了解log，我们可以用多项式来看一个例子：

```python
>>> dist = MultinomialDistribution(np.array([0.25, 0.25, 0.25, 0.25]))
>>> dist.log_pmf(np.array([1000, 0, 0, 0])
-1386.2943611198905
>>> dist.log_pmf(np.array([999, 0, 0, 0])
-1384.9080667587707
```

这种情况下，我们得到极小的概率（比我们上面讨论的微小的值小得多）。这是因为PMF中的分数巨大：由于溢出不能计算1000因子。但是，阶乘的log可以是：

```python
>>> from scipy.special import gamma, gammaln
>>> gamma(1000 + 1)
inf
>>> gammaln(1000 + 1)
5912.1281784881639
```

如果我们试图使用`gamma`函数计算PMF，那么我们最终会得到`gamma(1000 + 1) / gamma(1000 + 1)`，这生成`nan`值（即使我们可以看到它应该是 1）。但是，因为我们用对数进行计算，这不是一个问题，我们不需要担心！

## Sampling Magical Items, Revisited

既然我们已经写了多项式函数，我们可以让它来产生Magical Items。为此，创建`MagicItemDistribution`类，位于文件`rpg.py`中：

```python
class MagicItemDistribution(object):

    # these are the names (and order) of the stats that all magical
    # items will have
    stats_names = ("dexterity", "constitution", "strength",
                   "intelligence", "wisdom", "charisma")

    def __init__(self, bonus_probs, stats_probs, rso=np.random):
        """Initialize a magic item distribution parameterized by `bonus_probs`
        and `stats_probs`.

        Parameters
        ----------
        bonus_probs: numpy array of length m
            The probabilities of the overall bonuses. Each index in
            the array corresponds to the bonus of that amount (e.g.,
            index 0 is +0, index 1 is +1, etc.)

        stats_probs: numpy array of length 6
            The probabilities of how the overall bonus is distributed
            among the different stats. `stats_probs[i]` corresponds to
            the probability of giving a bonus point to the ith stat;
            i.e., the value at `MagicItemDistribution.stats_names[i]`.

        rso: numpy RandomState object (default: np.random)
            The random number generator

        """
        # Create the multinomial distributions we'll be using
        self.bonus_dist = MultinomialDistribution(bonus_probs, rso=rso)
        self.stats_dist = MultinomialDistribution(stats_probs, rso=rso)
```

`MagicItemDistribution`类的构造函数获取概率，统计概率和随机数生成器的参数。即使我们指出了想要的概率，通常将参数编码为传入的参数，这样可以在不同的分布下对项目进行抽样。（例如，奖金概率可能随着玩家的等级增加而改变。）我们将统计数据的名称编码为类变量`stats_names`，尽管这可能成为构造函数的另一个参数。

如上所述，两个步骤来对magic item进行抽样：首先对整体奖金进行抽样，然后对统计数据中的奖金进行抽样。因此，我们将这些步骤编码为两种方法：`_sample_bonus`和`_sample_stats`：

```python
def _sample_bonus(self):
    """Sample a value of the overall bonus.

    Returns
    -------
    integer
        The overall bonus

    """
    # The bonus is essentially just a sample from a multinomial
    # distribution with n=1; i.e., only one event occurs.
    sample = self.bonus_dist.sample(1)

    # `sample` is an array of zeros and a single one at the
    # location corresponding to the bonus. We want to convert this
    # one into the actual value of the bonus.
    bonus = np.argmax(sample)
    return bonus

def _sample_stats(self):
    """Sample the overall bonus and how it is distributed across the
    different stats.

    Returns
    -------
    numpy array of length 6
        The number of bonus points for each stat

    """
    # First we need to sample the overall bonus
    bonus = self._sample_bonus()

    # Then, we use a different multinomial distribution to sample
    # how that bonus is distributed. The bonus corresponds to the
    # number of events.
    stats = self.stats_dist.sample(bonus)
    return stats
```

我们可以合并这些方法——特别是因为`_sample_stats`是唯一依赖于`_sample_bonus`的函数，但是我将它们分开，因为它使得抽样程序更易于理解，并且将其分解成较小的部分代码更容易测试。

n 还会注意到，这些方法前缀为下划线，表示它们并不在课外使用。相反，我们提供`sample`函数示例：

```python
def sample(self):
    """Sample a random magical item.

    Returns
    -------
    dictionary
        The keys are the names of the stats, and the values are
        the bonus conferred to the corresponding stat.

    """
    stats = self._sample_stats()
    item_stats = dict(zip(self.stats_names, stats))
    return item_stats
```

示例函数基本上与`_sample_stats`相同，除了它返回一个以`stats`为键的字典。它提供了一个干净可理解的接口，用于抽样项目，哪些统计数据有多少奖励积分 —— 但是如果需要采样更多并且要求效率，它还会保留使用`_sample_stats`的选项。

我们用类似的设计来评估项目的概率。再次，我们公开了`pmf`和`log_pmf`，它们采用`sample`生成的字典：

```python
def log_pmf(self, item):
    """Compute the log probability of the given magical item.

    Parameters
    ----------
    item: dictionary
        The keys are the names of the stats, and the values are
        the bonuses conferred to the corresponding stat.

    Returns
    -------
    float
        The value corresponding to log(p(item))

    """
    # First pull out the bonus points for each stat, in the
    # correct order, then pass that to _stats_log_pmf.
    stats = np.array([item[stat] for stat in self.stats_names])
    log_pmf = self._stats_log_pmf(stats)
    return log_pmf

def pmf(self, item):
    """Compute the probability the given magical item.

    Parameters
    ----------
    item: dictionary
        The keys are the names of the stats, and the values are
        the bonus conferred to the corresponding stat.

    Returns
    -------
    float
        The value corresponding to p(item)

    """
    return np.exp(self.log_pmf(item))
```

这些方法依赖于`_stats_log_pmf`，它计算统计的概率（但是采用数组而不是字典）：

```python
def _stats_log_pmf(self, stats):
    """Evaluate the log-PMF for the given distribution of bonus points
    across the different stats.

    Parameters
    ----------
    stats: numpy array of length 6
        The distribution of bonus points across the stats

    Returns
    -------
    float
        The value corresponding to log(p(stats))

    """
    # There are never any leftover bonus points, so the sum of the
    # stats gives us the total bonus.
    total_bonus = np.sum(stats)

    # First calculate the probability of the total bonus
    logp_bonus = self._bonus_log_pmf(total_bonus)

    # Then calculate the probability of the stats
    logp_stats = self.stats_dist.log_pmf(stats)

    # Then multiply them together (using addition, because we are
    # working with logs)
    log_pmf = logp_bonus + logp_stats
    return log_pmf
```

`_stats_log_pmf`反过来依赖于`_bonus_log_pmf`，计算总奖金的概率：

```python
def _bonus_log_pmf(self, bonus):
    """Evaluate the log-PMF for the given bonus.

    Parameters
    ----------
    bonus: integer
        The total bonus.

    Returns
    -------
    float
        The value corresponding to log(p(bonus))

    """
    # Make sure the value that is passed in is within the
    # appropriate bounds
    if bonus < 0 or bonus >= len(self.bonus_dist.p):
        return -np.inf

    # Convert the scalar bonus value into a vector of event
    # occurrences
    x = np.zeros(len(self.bonus_dist.p))
    x[bonus] = 1

    return self.bonus_dist.log_pmf(x)
```

我们现在可以创建我们的分布如下：

```python
>>> import numpy as np
>>> from rpg import MagicItemDistribution
>>> bonus_probs = np.array([0.0, 0.55, 0.25, 0.12, 0.06, 0.02])
>>> stats_probs = np.ones(6) / 6.0
>>> rso = np.random.RandomState(234892)
>>> item_dist = MagicItemDistribution(bonus_probs, stats_probs, rso=rso)
```

一旦创建，我们可以使用它来生成几个不同的items：

```python
>>> item_dist.sample()
{'dexterity': 0, 'strength': 0, 'constitution': 0, 
 'intelligence': 0, 'wisdom': 0, 'charisma': 1}
>>> item_dist.sample()
{'dexterity': 0, 'strength': 0, 'constitution': 1, 
 'intelligence': 0, 'wisdom': 2, 'charisma': 0}
>>> item_dist.sample()
{'dexterity': 1, 'strength': 0, 'constitution': 1, 
 'intelligence': 0, 'wisdom': 0, 'charisma': 0}
 ```

而且，如果我们想要，我们可以评估抽样项目的概率：

```python
>>> item = item_dist.sample()
>>> item
{'dexterity': 0, 'strength': 0, 'constitution': 0, 
 'intelligence': 0, 'wisdom': 2, 'charisma': 0}
>>> item_dist.log_pmf(item)
-4.9698132995760007
>>> item_dist.pmf(item)
0.0069444444444444441
```

## Estimating Attack Damage

我们已经看到一个采样应用：随机产生怪物掉落。我之前提到，当你想从整个分布中估计某些东西时，也可以使用采样，并且我们可以使用`MagicItemDistribution`来执行此操作。例如，假设我们的RPG中的伤害是通过滚动一些D12（十二面骰子）而起作用。玩家默认滚动一个死亡，然后根据他们的强度添加骰子。所以，例如，如果他们有一个+2的力量奖金，他们可以滚三个骰子。所造成的伤害是骰子的总和。

我们可能想知道玩家在找到一些武器后会遇到多少伤害？例如，作为设定怪物难度的因素。收集两件物品后，我们希望玩家能够在大约50％的战斗中击败三次命中的怪物。怪物有多少生命值？

回答这个问题的一个方法是通过抽样。我们可以使用以下方案：

1. 随机选择一个魔术物品。

2. 根据项目的奖金，计算攻击时滚动的骰子数。

3. 根据将要滚动的骰子的数量，为三次命中造成的伤害生成一个样本。
多次重复1-3步。这将导致近似损失分布。

**Implementing a Distribution Over Damage**

`DamageDistribution`类（也在`rpg.py`文件）如下：

```python
class DamageDistribution(object):

    def __init__(self, num_items, item_dist,
                 num_dice_sides=12, num_hits=1, rso=np.random):
        """Initialize a distribution over attack damage. This object can
        sample possible values for the attack damage dealt over
        `num_hits` hits when the player has `num_items` items, and
        where attack damage is computed by rolling dice with
        `num_dice_sides` sides.

        Parameters
        ----------
        num_items: int
            The number of items the player has.
        item_dist: MagicItemDistribution object
            The distribution over magic items.
        num_dice_sides: int (default: 12)
            The number of sides on each die.
        num_hits: int (default: 1)
            The number of hits across which we want to calculate damage.
        rso: numpy RandomState object (default: np.random)
            The random number generator

        """

        # This is an array of integers corresponding to the sides of a
        # single die.
        self.dice_sides = np.arange(1, num_dice_sides + 1)

        # Create a multinomial distribution corresponding to one of
        # these dice.  Each side has equal probabilities.
        self.dice_dist = MultinomialDistribution(
            np.ones(num_dice_sides) / float(num_dice_sides), rso=rso)

        self.num_hits = num_hits
        self.num_items = num_items
        self.item_dist = item_dist

    def sample(self):
        """Sample the attack damage.

        Returns
        -------
        int
            The sampled damage

        """
        # First, we need to randomly generate items (the number of
        # which was passed into the constructor).
        items = [self.item_dist.sample() for i in xrange(self.num_items)]

        # Based on the item stats (in particular, strength), compute
        # the number of dice we get to roll.
        num_dice = 1 + np.sum([item['strength'] for item in items])

        # Roll the dice and compute the resulting damage.
        dice_rolls = self.dice_dist.sample(self.num_hits * num_dice)
        damage = np.sum(self.dice_sides * dice_rolls)
        return damage
```

构造函数将骰子的边数作为参数，我们想要计算伤害的命中次数，玩家有多少items，magic items（`MagicItemDistribution`类）和随机状态对象的分布。默认情况下，我们将`num_dice_sides`设置为12，因为它是一个参数，不太可能改变。同样，我们将`num_hits`设置为1作为默认值，因为更有可能的用例是我们只想对单个命中采样。

然后我们在`sample中实现采样逻辑。（注意与`MagicItemDistribution`的结构相似性。）首先，我们生成一组可能的玩家的magic items。然后，我们来看一下这些项目的统计数据，并从中计算出骰子数量。最后，我们滚动骰子（再次依赖于我们的信任多项式函数）并计算出损失。

**评估概率**

您可能已经注意到，我们的`DamageDistribution`中没有包含`log_pmf`或`pmf`函数。这是因为我们实际上并不知道PMF是什么！这是等式：

\[ \sum_{{item}_1, \ldots{}, {item}_m} p(\mathrm{damage} \vert \mathrm{item}_1,\ldots{},\mathrm{item}_m)p(\mathrm{item}_1)\cdots{}p(\mathrm{item}_m) \]

这个方程式表示，我们需要计算每个可能的损伤量的概率，给出每个可能的一组
(m\)项。我们实际上可以通过暴力计算，但不好。这是一个完美的例子，我们想使用抽样将解决方案近似于一个我们无法精确计算的问题。所以，不使用PMF，我们将在下一节中显示如何使用样本近似分布。

## 近似分布

现在我们有机会回答：如果玩家有两件物品，而且我们希望玩家能够在50％的时间内击败三只怪物，那么怪物有多少个生命值呢？

首先，我们使用与之前创建的相同`item_dist`和`rso`创建我们的分布对象：

```python
>>> from rpg import DamageDistribution
>>> damage_dist = DamageDistribution(2, item_dist, num_hits=3, rso=rso)
```

现在我们可以绘制一堆样品，并计算第50个百分位数（大于50％样品的损伤值）：

```python
>>> samples = np.array([damage_dist.sample() for i in xrange(100000)])
>>> samples.min()
3
>>> samples.max()
154
>>> np.percentile(samples, 50)
27.0
```

如果我们绘制一个直方图，得到了每个损伤的样本数量，它将如图18.1所示。

![](/images/damage_distribution.png)

玩家可能会造成很大的伤害，但是它有一个长尾巴：第50个百分点是27分，这意味着在一半的样品中，玩家造成不超过27点的伤害。因此，如果我们想使用这个标准来设定怪物的难度，我们会给他们27个生命值。

## 总结

在本章中，我们已经看到如何编写用于从非标准概率分布生成样本的代码，以及如何计算这些样本的概率。在这个例子中，我们介绍了一些适用于一般情况的设计决策：

1. 使用类表示概率分布，包括用于采样和评估PMF（或PDF）的函数。

2. 使用对数计算PMF（或PDF）。

3. 从随机数发生器对象生成样本以实现可重现的随机性。

4. 输入/输出是清晰可理解的（例如，使用词典作为`MagicItemDistribution.sample`的输出），同时仍暴露出这些函数（例如`MagicItemDistribution._sample_stats`）不那么清晰但更有效。
另外，概率分布的抽样对于生成单个随机值（例如，在击败怪物之后产生单个魔法物品）和计算关于我们不知道的分布的信息（例如，发现两个玩家有可能造成多少伤害）。你可能遇到的每种类型的抽样都属于这两类之一。差异仅与你正在从哪个版本中抽取有关。代码独立于这些分布——保持不变。
asks += 1
        # ... store the item ...

    def task_done(self):
        self._unfinished_tasks -= 1
        if self._unfinished_tasks == 0:
            self._join_future.set_result(None)

    @asyncio.coroutine
    def join(self):
        if self._unfinished_tasks > 0:
            yield from self._join_future
```